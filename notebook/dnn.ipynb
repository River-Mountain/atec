{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eager execution dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', filename='./log/dnn_0515.log', level=logging.INFO)\n",
    "\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "def parse_csv(line):\n",
    "    a = [[0.0]] * 422\n",
    "    b = [[0]]\n",
    "    b = b + a\n",
    "    example_defaults = b  # sets field types\n",
    "    parsed_line = tf.decode_csv(line, example_defaults)\n",
    "    # First 4 fields are features, combine into single tensor\n",
    "    features = tf.reshape(parsed_line[1:], shape=(422,))\n",
    "    # Last field is the label\n",
    "    label = tf.reshape(parsed_line[0], shape=())\n",
    "    return features, label\n",
    "\n",
    "filenames = [\"/home/mountain/atec/data/0514/train_feature00\", \"/home/mountain/atec/data/0514/train_feature01\", \"/home/mountain/atec/data/0514/train_feature02\", \"/home/mountain/atec/data/0514/train_feature03\", \"/home/mountain/atec/data/0514/train_feature04\", \"/home/mountain/atec/data/0514/train_feature05\", \"/home/mountain/atec/data/0514/train_feature06\", \"/home/mountain/atec/data/0514/train_feature07\", \"/home/mountain/atec/data/0514/train_feature08\", \"/home/mountain/atec/data/0514/train_feature09\", \"/home/mountain/atec/data/0514/train_feature10\", \"/home/mountain/atec/data/0514/train_feature11\", \"/home/mountain/atec/data/0514/train_feature12\", \"/home/mountain/atec/data/0514/train_feature13\", \"/home/mountain/atec/data/0514/train_feature14\", \"/home/mountain/atec/data/0514/train_feature15\", \"/home/mountain/atec/data/0514/train_feature16\", \"/home/mountain/atec/data/0514/train_feature17\", \"/home/mountain/atec/data/0514/train_feature18\", \"/home/mountain/atec/data/0514/train_feature19\", \"/home/mountain/atec/data/0514/train_feature20\", \"/home/mountain/atec/data/0514/train_feature21\", \"/home/mountain/atec/data/0514/train_feature22\", \"/home/mountain/atec/data/0514/train_feature23\", \"/home/mountain/atec/data/0514/train_feature24\", \"/home/mountain/atec/data/0514/train_feature25\", \"/home/mountain/atec/data/0514/train_feature26\", \"/home/mountain/atec/data/0514/train_feature27\", \"/home/mountain/atec/data/0514/train_feature28\", \"/home/mountain/atec/data/0514/train_feature29\", \"/home/mountain/atec/data/0514/train_feature30\", \"/home/mountain/atec/data/0514/train_feature31\", \"/home/mountain/atec/data/0514/train_feature32\", \"/home/mountain/atec/data/0514/train_feature33\", \"/home/mountain/atec/data/0514/train_feature34\", \"/home/mountain/atec/data/0514/train_feature35\", \"/home/mountain/atec/data/0514/train_feature36\", \"/home/mountain/atec/data/0514/train_feature37\", \"/home/mountain/atec/data/0514/train_feature38\", \"/home/mountain/atec/data/0514/train_feature39\", \"/home/mountain/atec/data/0514/train_feature40\", \"/home/mountain/atec/data/0514/train_feature41\", \"/home/mountain/atec/data/0514/train_feature42\", \"/home/mountain/atec/data/0514/train_feature43\", \"/home/mountain/atec/data/0514/train_feature44\", \"/home/mountain/atec/data/0514/train_feature45\", \"/home/mountain/atec/data/0514/train_feature46\", \"/home/mountain/atec/data/0514/train_feature47\", \"/home/mountain/atec/data/0514/train_feature48\", \"/home/mountain/atec/data/0514/train_feature49\", \"/home/mountain/atec/data/0514/train_feature50\", \"/home/mountain/atec/data/0514/train_feature51\", \"/home/mountain/atec/data/0514/train_feature52\", \"/home/mountain/atec/data/0514/train_feature53\", \"/home/mountain/atec/data/0514/train_feature54\", \"/home/mountain/atec/data/0514/train_feature55\", \"/home/mountain/atec/data/0514/train_feature56\", \"/home/mountain/atec/data/0514/train_feature57\", \"/home/mountain/atec/data/0514/train_feature58\", \"/home/mountain/atec/data/0514/train_feature59\", \"/home/mountain/atec/data/0514/train_feature60\", \"/home/mountain/atec/data/0514/train_feature61\", \"/home/mountain/atec/data/0514/train_feature62\", \"/home/mountain/atec/data/0514/train_feature63\", \"/home/mountain/atec/data/0514/train_feature64\", \"/home/mountain/atec/data/0514/train_feature65\", \"/home/mountain/atec/data/0514/train_feature66\"]\n",
    "\n",
    "train_dataset = tf.data.TextLineDataset(filenames)\n",
    "train_dataset = train_dataset.map(parse_csv)      # parse each row\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10001)  # randomize\n",
    "train_dataset = train_dataset.batch(256)\n",
    "\n",
    "# View a single example entry from a batch\n",
    "features, label = tfe.Iterator(train_dataset).next()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(256, activation=\"relu\", input_shape=(422,)),\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    y = tf.reshape(y, [-1, 1])\n",
    "    return tf.losses.log_loss(labels=y, predictions=y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tfe.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.005)\n",
    "\n",
    "prediction = []\n",
    "label = []\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    logging.info(\"epoch: %s\" % epoch)\n",
    "#     print epoch\n",
    "    epoch_loss_avg = tfe.metrics.Mean()\n",
    "\n",
    "    i = 0\n",
    "    for x, y in tfe.Iterator(train_dataset):\n",
    "        # Optimize the model\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            logging.info(\"i: %s\" % i)\n",
    "#             print i\n",
    "        grads = grad(model, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                              global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "        # Track progress\n",
    "        epoch_loss_avg(loss(model, x, y))  # add current batch loss\n",
    "        \n",
    "\n",
    "    logging.info(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch, epoch_loss_avg.result()))\n",
    "#     print epoch_loss_avg.result()\n",
    "\n",
    "checkpoint_prefix = \"./model/dnn_0514\"\n",
    "root = tfe.Checkpoint(optimizer=optimizer,\n",
    "                      model=model,\n",
    "                      optimizer_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "root.save(file_prefix=checkpoint_prefix)\n",
    "# or\n",
    "# root.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练时间\n",
    "1. 一轮4个小时"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras dnn（学习模仿）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers import Conv1D,MaxPooling2D,Convolution1D\n",
    "from keras.optimizers import SGD,Adam,RMSprop\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, normalization\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K # turn image into speficifc shape\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "# load data\n",
    "train_labeled = pd.read_hdf(\"train_labeled.h5\", \"train\")\n",
    "train_unlabeled = pd.read_hdf(\"train_unlabeled.h5\", \"train\")\n",
    "test = pd.read_hdf(\"test.h5\", \"test\")\n",
    "y_train_original = train_labeled.values[2000:,0]\n",
    "X_train_original = train_labeled.values[2000:,1:129]\n",
    "y_val =  train_labeled.values[0:2000,0]\n",
    "X_val =  train_labeled.values[0:2000,1:129]\n",
    "X_unlabeled = train_unlabeled.values[:,0:128]\n",
    "y_train_original = keras.utils.to_categorical(y_train_original,10)\n",
    "y_val =  keras.utils.to_categorical(y_val,10)\n",
    "X_valid = test.values[:,0:128]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(normalization.BatchNormalization(input_shape=(128,)))\n",
    "model.add(Dense(500, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(400, activation = 'relu'))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(200, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, momentum=0.9, decay=0.01, nesterov=True)\n",
    "rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "#adam = optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=sgd,\n",
    "#               metrics=['accuracy'])\n",
    "adam = Adam()\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "acc_last = 0\n",
    "acc_now = 1\n",
    "\n",
    "print(\"====last_time===\")\n",
    "model = Sequential()\n",
    "model.add(normalization.BatchNormalization(input_shape=(128,)))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "adam = Adam()\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train_original, y_train_original, epochs=50, verbose=0, batch_size=100)  # 461 epch\n",
    "score = model.evaluate(X_val, y_val)\n",
    "print(\"==acc last = first ===\")\n",
    "acc_last = score[1]\n",
    "print(acc_last)\n",
    "\n",
    "# predict\n",
    "y_unlabelled = model.predict_proba(X_unlabeled, batch_size=32, verbose=0)\n",
    "y_add_train = model.predict_classes(X_unlabeled)\n",
    "y_max_p = y_unlabelled.max(axis=1)\n",
    "indice = y_max_p > (np.mean(y_max_p)+2.58 * np.std(y_max_p)/(len(y_max_p)**0.5))\n",
    "X_add_train = X_unlabeled[indice, :]\n",
    "y_add_train = keras.utils.to_categorical(y_add_train[indice], 10)\n",
    "X_train_original = np.concatenate((X_train_original, X_add_train), axis=0)\n",
    "y_train_original = np.concatenate((y_train_original, y_add_train), axis=0)\n",
    "print(y_train_original.shape)\n",
    "X_unlabeled = X_unlabeled[~indice,:]\n",
    "\n",
    "\n",
    "acc_now  =  acc_last\n",
    "print(\"==acc now = first ===\")\n",
    "print(acc_now)\n",
    "i = 0\n",
    "while acc_now >= acc_last:\n",
    "    # y_train_original = train_labeled.values[2000:, 0]\n",
    "    # X_train_original = train_labeled.values[2000:, 1:129]\n",
    "    acc_last = acc_now\n",
    "    print(\"traing data size\")\n",
    "    print(y_train_original.shape)\n",
    "    print(\"====update=====\")\n",
    "    model = Sequential()\n",
    "    model.add(normalization.BatchNormalization(input_shape=(128,)))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(400, activation='relu'))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    adam = Adam()\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X_train_original, y_train_original, epochs=50+10*i, verbose=0, batch_size=100)  # 461 epch\n",
    "    score = model.evaluate(X_val, y_val)\n",
    "    acc_now = score[1]\n",
    "\n",
    "    # predict\n",
    "    y_unlabelled = model.predict_proba(X_unlabeled, batch_size=32, verbose=0)\n",
    "    y_add_train = model.predict_classes(X_unlabeled)\n",
    "    y_max_p = y_unlabelled.max(axis=1)\n",
    "    indice = y_max_p > (np.mean(y_max_p) + 2.58 * np.std(y_max_p)/(len(y_max_p**0.5)))\n",
    "    X_add_train = X_unlabeled[indice, :]\n",
    "    y_add_train = keras.utils.to_categorical(y_add_train[indice], 10)\n",
    "    X_train_original = np.concatenate((X_train_original, X_add_train), axis=0)\n",
    "    y_train_original = np.concatenate((y_train_original, y_add_train), axis=0)\n",
    "    print(\"training data now\")\n",
    "    print(y_train_original.shape)\n",
    "    if len(X_unlabeled[~indice, :]) == 0:\n",
    "        break\n",
    "    else:\n",
    "        X_unlabeled = X_unlabeled[~indice, :]\n",
    "    print(\"acc last---  acc now\")\n",
    "    print(acc_last, acc_now)\n",
    "    if acc_now<=acc_last:\n",
    "        break\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "    y_valid = model.predict_classes(X_valid)\n",
    "    result = pd.DataFrame(columns=['Id', 'y'])\n",
    "    result['y'] = np.transpose(y_valid)\n",
    "    result['Id'] = range(30000, 38000)\n",
    "    result.to_csv('labelled'+str(i)+'.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
