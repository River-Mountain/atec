{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rbf, tensorflow相似度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tf.executing_eagerly()\n",
    "\n",
    "### 先整体填充nan为均值，整体标准化，再把-1和1的样本拿出来的\n",
    "\n",
    "data_train = pd.read_pickle(\"/home/mountain/atec/data/raw_pickle/train\")\n",
    "\n",
    "def data_fillna_mean(data):\n",
    "    # id, label, date要去掉\n",
    "    for feature in data.columns:\n",
    "        if feature not in [\"id\", \"label\", \"date\"]:\n",
    "            print feature\n",
    "            data[feature] = data[feature].fillna(data[feature].mean())\n",
    "\n",
    "data_fillna_mean(data_train)\n",
    "\n",
    "def data_scale(data_train_feature):\n",
    "    for feature in data_train_feature.columns:\n",
    "        if feature not in [\"id\", \"label\", \"date\"]:\n",
    "            print feature\n",
    "            scaler = MinMaxScaler()\n",
    "            train_scaler = scaler.fit(data_train_feature[feature].values.reshape(-1, 1))\n",
    "            train_transform = scaler.transform(data_train_feature[feature].values.reshape(-1, 1))\n",
    "            data_train_feature[feature] = pd.DataFrame(train_transform, dtype=np.float32)\n",
    "\n",
    "data_scale(data_train)\n",
    "\n",
    "data_train_pos_1 = data_train[data_train[\"label\"] == 1].drop(labels=[\"id\", \"label\", \"date\"], axis=1)\n",
    "data_train_neg_1 = data_train[data_train[\"label\"] == -1].drop(labels=[\"id\", \"label\", \"date\"], axis=1)\n",
    "\n",
    "data_train_pos_1.to_pickle(\"../data/raw_pickle_split/train_feature_pos_1\")\n",
    "data_train_neg_1.to_pickle(\"../data/raw_pickle_split/train_feature_neg_1\")\n",
    "\n",
    "data_train_pos_1 = pd.read_pickle(\"../data/raw_pickle_split/train_feature_pos_1\")\n",
    "data_train_neg_1 = pd.read_pickle(\"../data/raw_pickle_split/train_feature_neg_1\")\n",
    "\n",
    "### 计算欧氏距离\n",
    "\n",
    "A = tf.constant(data_train_neg_1.values)\n",
    "B = tf.constant(data_train_pos_1.values)\n",
    "\n",
    "A_Squ_A_Sum = tf.reduce_sum(A*A, 1)\n",
    "A_Squ_A_Sum = tf.reshape(A_Squ_A_Sum, [-1, 1])\n",
    "B_Squ_B_Sum = tf.reduce_sum(B*B, 1)\n",
    "B_Squ_B_Sum = tf.reshape(B_Squ_B_Sum, [-1, 1])\n",
    "\n",
    "# 这里Dist是平方了\n",
    "Dist = (A_Squ_A_Sum - 2 * tf.matmul(A, tf.transpose(B)) + tf.transpose(B_Squ_B_Sum))\n",
    "\n",
    "Dist_Min = tf.reduce_min(Dist, axis=1)\n",
    "Dist_Min_Index = np.array(tf.argmin(Dist, axis=1))\n",
    "\n",
    "Dist_Min_New = []\n",
    "for _ in Dist_Min:\n",
    "    Dist_Min_New.append(float(_))\n",
    "\n",
    "Dist_Min_Index_New = []\n",
    "for _ in Dist_Min_Index:\n",
    "    Dist_Min_Index_New.append(int(data_train_pos_1.iloc[_].name))\n",
    "\n",
    "### 再变成rbf\n",
    "\n",
    "import math\n",
    "# 欧氏距离转换为对应的相似度\n",
    "data_sim = [math.exp(-1 / 2.0 * d) for d in Dist_Min_New]\n",
    "\n",
    "### 生成结果\n",
    "\n",
    "res = []\n",
    "for i in range(len(data_sim)):\n",
    "    res_single = []\n",
    "    neg_index = data_train_neg_1.index[i]\n",
    "    pos_index = Dist_Min_Index_New[i]\n",
    "    sim = data_sim[i]\n",
    "    res_single = [neg_index, pos_index, sim]\n",
    "    res.append(res_single)\n",
    "\n",
    "# 这里的index只最原始的所有数据的行索引\n",
    "res = pd.DataFrame(res, columns=[\"neg_index\", \"pos_index\", \"rbf_sim\"])\n",
    "\n",
    "res.to_pickle(\"../data/rbf_sim\")\n",
    "\n",
    "res = pd.read_pickle(\"../data/rbf_sim\")\n",
    "\n",
    "(res[\"rbf_sim\"] < 0.1).sum()\n",
    "\n",
    "### 验证\n",
    "\n",
    "neg_row = data_train_neg_1.loc[2331]\n",
    "pos_row = data_train_pos_1.loc[270516]\n",
    "\n",
    "from scipy import spatial\n",
    "d = spatial.distance.euclidean(neg_row, pos_row)\n",
    "print math.exp(-1.0 / 2 * math.pow(d, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用tensorflow计算相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算欧氏距离，采用分治"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_neg_1 = pd.read_pickle(\"../data/tmp/train_feature_-1\")\n",
    "train_feature_0_1 = pd.read_pickle(\"../data/tmp/train_feature_0_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据分治"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_list = np.arange(0, train_feature_0_1.shape[0]+10000, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分别计算欧氏距离，取最小值，再合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dist_Min_All = []\n",
    "Dist_Min_Index_All = []\n",
    "\n",
    "for i in tqdm(range(len(split_list)-1)):\n",
    "    train_feature_0_1_split = train_feature_0_1.iloc[split_list[i]:split_list[i+1]]\n",
    "\n",
    "    A = tf.constant(train_feature_neg_1.values)\n",
    "    B = tf.constant(train_feature_0_1_split.values)\n",
    "\n",
    "    A_Squ_A_Sum = tf.reduce_sum(A*A, 1)\n",
    "    A_Squ_A_Sum = tf.reshape(A_Squ_A_Sum, [-1, 1])\n",
    "    B_Squ_B_Sum = tf.reduce_sum(B*B, 1)\n",
    "    B_Squ_B_Sum = tf.reshape(B_Squ_B_Sum, [-1, 1])\n",
    "\n",
    "    Dist = tf.sqrt((A_Squ_A_Sum - 2 * tf.matmul(A, tf.transpose(B)) + tf.transpose(B_Squ_B_Sum)))\n",
    "        \n",
    "    Dist_Min = tf.reduce_min(Dist, axis=1)\n",
    "    Dist_Min_Index = np.array(tf.argmin(Dist, axis=1))\n",
    "    \n",
    "    Dist_Min_New = []\n",
    "    for _ in Dist_Min:\n",
    "        Dist_Min_New.append(float(_))\n",
    "    \n",
    "    Dist_Min_Index_New = []\n",
    "    for _ in Dist_Min_Index:\n",
    "        Dist_Min_Index_New.append(int(train_feature_0_1_split.iloc[_].name))\n",
    "    Dist_Min_All.append(Dist_Min_New)\n",
    "    Dist_Min_Index_All.append(Dist_Min_Index_New)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并之后，再取最小值，注意index的变化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dist = pd.DataFrame(Dist_Min_All)\n",
    "data_dist_index = pd.DataFrame(Dist_Min_Index_All)\n",
    "\n",
    "data_dist_min = data_dist.min(axis=0)\n",
    "data_dist_min_index = data_dist.idxmin(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### index转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index转换\n",
    "data_dist_min_index_new = []\n",
    "for _ in data_dist_min_index.index:\n",
    "    print _\n",
    "    column_index = data_dist_index[_]\n",
    "    min_index = data_dist_min_index[_]\n",
    "    min_index_new = column_index[min_index]\n",
    "    data_dist_min_index_new.append(min_index_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欧氏距离转换为对应的相似度\n",
    "data_sim = [1.0 / (1.0 + d) for d in data_dist_min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(len(data_dist_min)):\n",
    "    res_single = []\n",
    "    neg_index = train_feature_neg_1.index[i]\n",
    "    pos_index = data_dist_min_index_new[i]\n",
    "    sim = data_sim[i]\n",
    "    res_single = [neg_index, pos_index, sim]\n",
    "    res.append(res_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里的index只最原始的所有数据的行索引\n",
    "res = pd.DataFrame(res, columns=[\"neg_index\", \"pos_index\", \"euc_sim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_pickle(\"../data/euc_similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "d = spatial.distance.euclidean(neg_row, pos_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 传统方法计算相似度（卡死）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', filename='./log/euc_dist.log', level=logging.INFO)\n",
    "\n",
    "data_train_feature_neg_1 = pd.read_pickle(\"../data/train_feature_-1\")\n",
    "data_train_feature_0_1 = pd.read_pickle(\"../data/train_feature_0_1\")\n",
    "data_train_label = pd.read_pickle(\"../data/train_label\")\n",
    "\n",
    "res_ndarr = np.empty((0,4))\n",
    "\n",
    "for neg_row_index, neg_row in data_train_feature_neg_1.iterrows():\n",
    "    \n",
    "    logging.info(\"    neg_row_index: %s\", neg_row_index)\n",
    "    \n",
    "    euc_dist = 999999\n",
    "    opt_index = 0\n",
    "    for pos_row_index, pos_row in data_train_feature_0_1.iterrows():\n",
    "        \n",
    "        if pos_row_index % 100000 == 0:\n",
    "            logging.info(\"    pos_row_index: %s\", pos_row_index)\n",
    "\n",
    "        d = spatial.distance.euclidean(neg_row, pos_row)\n",
    "        if d < euc_dist:\n",
    "            euc_dist = d\n",
    "            opt_index = pos_row_index\n",
    "    euc_dist_sim = 1.0 / (1.0 + d)\n",
    "    opt_index_label = data_train_label[\"label\"][opt_index]\n",
    "    res_ndarr = np.append(res_ndarr, np.array([[neg_row_index, opt_index, euc_dist_sim, opt_index_label]]), axis=0)\n",
    "\n",
    "res_ndarr.dump(\"../data/similarity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
