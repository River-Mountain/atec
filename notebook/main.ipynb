{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import spatial\n",
    "from scipy.interpolate import spline\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据初步处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_pickle(\"/home/mountain/atec/data/raw_pickle/train\")\n",
    "data_test = pd.read_pickle(\"/home/mountain/atec/data/raw_pickle/test_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = data_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_feature = pd.read_pickle(\"/home/mountain/atec/data/raw_pickle_split/train_feature\")\n",
    "data_test_feature = pd.read_pickle(\"/home/mountain/atec/data/raw_pickle_split/test_feature\")\n",
    "\n",
    "print data_train_feature.info()\n",
    "print data_test_feature.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据类型转换降低内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float, int转换数据格式，降低内存\n",
    "def dtype_descend(data):\n",
    "    data_float = data.select_dtypes(include=['float'])\n",
    "    data_converted_float = data_float.apply(pd.to_numeric, downcast='float')\n",
    "    for column in data_converted_float.columns:\n",
    "        print \"float \", column\n",
    "        data[column] = data_converted_float[column]\n",
    "    \n",
    "    data_int = data.select_dtypes(include=['int'])\n",
    "    data_converted_int = data_int.apply(pd.to_numeric, downcast='unsigned')\n",
    "    for column in data_converted_int.columns:\n",
    "        print \"int \", column\n",
    "        data[column] = data_converted_int[column]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_feature_desc = dtype_descend(data_test_feature)\n",
    "\n",
    "data_test_feature_desc.to_pickle(\"../data/0514/test_feature_filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label, id, date和feature拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train(data, data_name):\n",
    "    data_label = data[\"label\"]\n",
    "    data_label.to_pickle(\"../data/%s_label\" % (data_name))\n",
    "    data_date = data[\"date\"]\n",
    "    data_date.to_pickle(\"../data/%s_date\" % (data_name))\n",
    "    data_feature = data.drop(labels=[\"date\", \"id\", \"label\"], axis=1)\n",
    "    data_feature.to_pickle(\"../data/%s_feature\" % (data_name))\n",
    "\n",
    "def split_test(data, data_name):\n",
    "    data_date = data[\"date\"]\n",
    "    data_date.to_pickle(\"../data/%s_date\" % (data_name))\n",
    "    data_feature = data.drop(labels=[\"date\", \"id\"], axis=1)\n",
    "    data_feature.to_pickle(\"../data/%s_feature\" % (data_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train(data_train, \"train\")\n",
    "\n",
    "split_test(data_test, \"test_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据标签分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train classified by label\n",
    "def label_classify(data):\n",
    "    print \"all  number: \", data.shape[0]\n",
    "    print \"label = 1  number: \", data[data[\"label\"] == 1].shape[0]\n",
    "    print \"label = 0  number: \", data[data[\"label\"] == 0].shape[0]\n",
    "    print \"label = -1  number: \", data[data[\"label\"] == -1].shape[0]\n",
    "    print \"negative sampel percentage: {:.2%}\".format(1.0 * data[data[\"label\"] == 1].shape[0] / data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_classify(data_train_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据时间转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_datetime(t):\n",
    "    return pd.to_datetime(str(t), format='%Y%m%d')\n",
    "\n",
    "def time_plot(data):\n",
    "    data[\"date\"] = data[\"date\"].apply(int_to_datetime)\n",
    "    data[\"date\"].groupby(data[\"date\"]).count().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征划分，分别处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征划分\n",
    "- 缺失率>0.2且取值个数不超过100的\n",
    "    - 做ohe，然后以0.1为方差阈值做筛选\n",
    "    - 其实有个问题，一旦取值个数很多了！就会导致ohe之后非常稀疏，这个时候特征方差肯定会非常非常小！！所以其实很多特征还是被舍去了！\n",
    "- 和缺失率>0.2且取值个数超过100的\n",
    "    - 直接去掉，没办法填充缺失值，反而还会带来很大的噪声\n",
    "- 其他的\n",
    "    - 均值填充\n",
    "- 注意千万不要出现错行，不然很难处理！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据特征的取值个数划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断每个feature的unique value count，决定是否对该特征one-hot encoding\n",
    "# 这里的unique_count是不包括nan的\n",
    "def unique_count(data):\n",
    "    data_unique_count = pd.Series()\n",
    "    for _ in data.columns:\n",
    "        data_unique_count[_] = data[_].unique().shape[0]\n",
    "    \n",
    "    return data_unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_unique_count = unique_count(data_train_feature)\n",
    "data_test_unique_count = unique_count(data_test_feature)\n",
    "\n",
    "# 这里都包含nan数据的，找取值个数不超过100的feature(包括nan)，之所以放的很大，是为了避免一些缺失率太大的特征（test里面缺60%，train里面缺20%，比如f100)\n",
    "data_train_unique_count_small = data_train_unique_count[data_train_unique_count <= 100]\n",
    "data_test_unique_count_small = data_test_unique_count[data_test_unique_count <= 100]\n",
    "\n",
    "# 选择train和test里面取值个数不超过100的feature，交集\n",
    "categorical_columns = list(set(data_train_unique_count_small.index) & set(data_test_unique_count_small.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把读取原始数据里面的int feature选取出来，也当做类别型变量，和前面的取并集\n",
    "data_train_raw = pd.read_csv(\"../data/raw_csv/train.csv\").drop(labels=[\"id\", \"label\", \"date\"], axis=1)\n",
    "int_columns = data_train_raw.select_dtypes(include=['int']).columns\n",
    "categorical_int_columns = list(set(categorical_columns) | set(int_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print data_train_unique_count_small.shape\n",
    "print data_test_unique_count_small.shape\n",
    "print len(categorical_columns)\n",
    "print len(categorical_int_columns)\n",
    "print categorical_int_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_columns = list(set(data_train_raw.columns) - set(categorical_int_columns))\n",
    "print len(numerical_columns)\n",
    "print numerical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据特征的取值个数划分结果\n",
    "1. int变量  \n",
    "['f1', 'f2', 'f3', 'f4', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11','f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19']\n",
    "\n",
    "2. 类别型变量 不论是否包括int <= 100  \n",
    "['f169', 'f168', 'f160', 'f167', 'f166', 'f41', 'f40', 'f42', 'f200', 'f201', 'f47', 'f46', 'f49', 'f48', 'f52', 'f191', 'f194', 'f195', 'f196', 'f197', 'f190', 'f53', 'f50', 'f193', 'f198', 'f199', 'f219', 'f59', 'f192', 'f221', 'f101', 'f100', 'f187', 'f186', 'f220', 'f184', 'f183', 'f182', 'f181', 'f180', 'f189', 'f188', 'f21', 'f20', 'f26', 'f25', 'f24', 'f29', 'f28', 'f2', 'f173', 'f138', 'f139', 'f132', 'f133', 'f131', 'f136', 'f137', 'f135', 'f202', 'f203', 'f30', 'f31', 'f32', 'f33', 'f34', 'f36', 'f37', 'f38', 'f39', 'f125', 'f124', 'f127', 'f126', 'f123', 'f129', 'f128', 'f3', 'f89', 'f88', 'f87', 'f155', 'f156', 'f157', 'f158', 'f159', 'f92', 'f93', 'f90', 'f91', 'f96', 'f97', 'f94', 'f95', 'f98', 'f99', 'f258', 'f257', 'f256', 'f255', 'f254', 'f18', 'f19', 'f12', 'f13', 'f10', 'f11', 'f16', 'f17', 'f14', 'f15', 'f1', 'f141', 'f140', 'f4', 'f6', 'f7', 'f8', 'f9', 'f267', 'f268', 'f269', 'f65', 'f63', 'f62', 'f61', 'f60', 'f222', 'f178', 'f179', 'f176', 'f177', 'f174', 'f175', 'f172', 'f185', 'f170', 'f171', 'f273', 'f272', 'f275', 'f274', 'f277', 'f276']\n",
    "\n",
    "3. 数值连续型变量 > 100  \n",
    "['f161', 'f163', 'f162', 'f165', 'f164', 'f297', 'f288', 'f289', 'f115', 'f284', 'f285', 'f286', 'f287', 'f280', 'f281', 'f282', 'f283', 'f204', 'f205', 'f43', 'f207', 'f45', 'f44', 'f110', 'f208', 'f209', 'f292', 'f291', 'f290', 'f56', 'f57', 'f215', 'f214', 'f118', 'f119', 'f114', 'f296', 'f295', 'f117', 'f293', 'f111', 'f112', 'f113', 'f217', 'f216', 'f54', 'f55', 'f213', 'f212', 'f211', 'f51', 'f210', 'f58', 'f218', 'f116', 'f109', 'f108', 'f107', 'f106', 'f105', 'f104', 'f103', 'f102', 'f223', 'f294', 'f226', 'f227', 'f224', 'f225', 'f228', 'f229', 'f23', 'f22', 'f27', 'f206', 'f130', 'f134', 'f239', 'f238', 'f235', 'f234', 'f237', 'f236', 'f231', 'f230', 'f233', 'f232', 'f85', 'f35', 'f241', 'f243', 'f121', 'f120', 'f122', 'f81', 'f80', 'f83', 'f82', 'f5', 'f248', 'f249', 'f240', 'f84', 'f242', 'f86', 'f244', 'f245', 'f246', 'f247', 'f150', 'f151', 'f152', 'f153', 'f154', 'f259', 'f253', 'f252', 'f251', 'f250', 'f143', 'f142', 'f147', 'f146', 'f145', 'f144', 'f149', 'f148', 'f266', 'f264', 'f265', 'f262', 'f263', 'f260', 'f261', 'f69', 'f68', 'f67', 'f66', 'f64', 'f271', 'f270', 'f279', 'f278', 'f78', 'f79', 'f74', 'f75', 'f76', 'f77', 'f70', 'f71', 'f72', 'f73']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据特征的缺失率划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nan_ratio(data):\n",
    "    data_nan_ratio = data.isnull().sum() / data.shape[0]\n",
    "    data_nan_ratio.hist(bins=np.arange(0, 1.05, 0.05), figsize=(5, 5))\n",
    "    data_nan_ratio = data_nan_ratio[data_nan_ratio > 0].sort_values(ascending=False)\n",
    "    \n",
    "    return data_nan_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nan_ratio = get_nan_ratio(data_train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_nan_ratio = get_nan_ratio(data_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nan_set = set(train_nan_ratio[train_nan_ratio > 0.2].index)\n",
    "test_nan_set = set(test_nan_ratio[test_nan_ratio > 0.2].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据特征的缺失率划分结果（>0.2）\n",
    "['f160', 'f297', 'f288', 'f289', 'f296', 'f284', 'f285', 'f286', 'f287', 'f280', 'f281', 'f282', 'f283', 'f41', 'f40', 'f43', 'f42', 'f45', 'f44', 'f47', 'f46', 'f49', 'f48', 'f292', 'f291', 'f290', 'f118', 'f119', 'f114', 'f115', 'f116', 'f117', 'f110', 'f111', 'f112', 'f113', 'f56', 'f57', 'f54', 'f55', 'f52', 'f53', 'f50', 'f51', 'f58', 'f59', 'f295', 'f109', 'f108', 'f107', 'f106', 'f105', 'f104', 'f103', 'f102', 'f101', 'f100', 'f294', 'f23', 'f22', 'f21', 'f20', 'f27', 'f26', 'f25', 'f24', 'f29', 'f28', 'f293', 'f138', 'f139', 'f132', 'f133', 'f130', 'f131', 'f136', 'f137', 'f134', 'f135', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f125', 'f124', 'f127', 'f126', 'f121', 'f120', 'f123', 'f122', 'f129', 'f128', 'f5', 'f89', 'f88', 'f85', 'f84', 'f87', 'f86', 'f81', 'f80', 'f83', 'f82', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f92', 'f93', 'f90', 'f91', 'f96', 'f97', 'f94', 'f95', 'f98', 'f99', 'f143', 'f142', 'f141', 'f140', 'f147', 'f146', 'f145', 'f144', 'f149', 'f148', 'f69', 'f68', 'f67', 'f66', 'f65', 'f64', 'f63', 'f62', 'f61', 'f60', 'f279', 'f278', 'f78', 'f79', 'f74', 'f75', 'f76', 'f77', 'f70', 'f71', 'f72', 'f73']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_set = set(['f160', 'f297', 'f288', 'f289', 'f296', 'f284', 'f285', 'f286', 'f287', 'f280', 'f281', 'f282', 'f283', 'f41', 'f40', 'f43', 'f42', 'f45', 'f44', 'f47', 'f46', 'f49', 'f48', 'f292', 'f291', 'f290', 'f118', 'f119', 'f114', 'f115', 'f116', 'f117', 'f110', 'f111', 'f112', 'f113', 'f56', 'f57', 'f54', 'f55', 'f52', 'f53', 'f50', 'f51', 'f58', 'f59', 'f295', 'f109', 'f108', 'f107', 'f106', 'f105', 'f104', 'f103', 'f102', 'f101', 'f100', 'f294', 'f23', 'f22', 'f21', 'f20', 'f27', 'f26', 'f25', 'f24', 'f29', 'f28', 'f293', 'f138', 'f139', 'f132', 'f133', 'f130', 'f131', 'f136', 'f137', 'f134', 'f135', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f125', 'f124', 'f127', 'f126', 'f121', 'f120', 'f123', 'f122', 'f129', 'f128', 'f5', 'f89', 'f88', 'f85', 'f84', 'f87', 'f86', 'f81', 'f80', 'f83', 'f82', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f92', 'f93', 'f90', 'f91', 'f96', 'f97', 'f94', 'f95', 'f98', 'f99', 'f143', 'f142', 'f141', 'f140', 'f147', 'f146', 'f145', 'f144', 'f149', 'f148', 'f69', 'f68', 'f67', 'f66', 'f65', 'f64', 'f63', 'f62', 'f61', 'f60', 'f279', 'f278', 'f78', 'f79', 'f74', 'f75', 'f76', 'f77', 'f70', 'f71', 'f72', 'f73'])\n",
    "categorical_set = set(['f169', 'f168', 'f160', 'f167', 'f166', 'f41', 'f40', 'f42', 'f200', 'f201', 'f47', 'f46', 'f49', 'f48', 'f52', 'f191', 'f194', 'f195', 'f196', 'f197', 'f190', 'f53', 'f50', 'f193', 'f198', 'f199', 'f219', 'f59', 'f192', 'f221', 'f101', 'f100', 'f187', 'f186', 'f220', 'f184', 'f183', 'f182', 'f181', 'f180', 'f189', 'f188', 'f21', 'f20', 'f26', 'f25', 'f24', 'f29', 'f28', 'f2', 'f173', 'f138', 'f139', 'f132', 'f133', 'f131', 'f136', 'f137', 'f135', 'f202', 'f203', 'f30', 'f31', 'f32', 'f33', 'f34', 'f36', 'f37', 'f38', 'f39', 'f125', 'f124', 'f127', 'f126', 'f123', 'f129', 'f128', 'f3', 'f89', 'f88', 'f87', 'f155', 'f156', 'f157', 'f158', 'f159', 'f92', 'f93', 'f90', 'f91', 'f96', 'f97', 'f94', 'f95', 'f98', 'f99', 'f258', 'f257', 'f256', 'f255', 'f254', 'f18', 'f19', 'f12', 'f13', 'f10', 'f11', 'f16', 'f17', 'f14', 'f15', 'f1', 'f141', 'f140', 'f4', 'f6', 'f7', 'f8', 'f9', 'f267', 'f268', 'f269', 'f65', 'f63', 'f62', 'f61', 'f60', 'f222', 'f178', 'f179', 'f176', 'f177', 'f174', 'f175', 'f172', 'f185', 'f170', 'f171', 'f273', 'f272', 'f275', 'f274', 'f277', 'f276'])\n",
    "# 做ohe的目的主要是为了转换缺失值，既要把选择缺失率>0.2的，又要选择取值个数不超过100的。缺失率很小的不值得ohe\n",
    "nan_categorical_set = nan_set & categorical_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 要做ohe的feature 缺失率>0.2 & 取值个数不超过100\n",
    "- ['f125', 'f124', 'f127', 'f126', 'f123', 'f160', 'f129', 'f128', 'f91', 'f140', 'f99', 'f101', 'f100', 'f41', 'f40', 'f42', 'f89', 'f88', 'f47', 'f46', 'f49', 'f48', 'f87', 'f21', 'f20', 'f26', 'f25', 'f24', 'f29', 'f28', 'f63', 'f62', 'f61', 'f65', 'f155', 'f156', 'f157', 'f158', 'f159', 'f92', 'f93', 'f138', 'f139', 'f96', 'f97', 'f94', 'f95', 'f132', 'f133', 'f98', 'f131', 'f136', 'f137', 'f135', 'f52', 'f53', 'f50', 'f90', 'f59', 'f30', 'f31', 'f32', 'f33', 'f34', 'f36', 'f37', 'f38', 'f39', 'f141', 'f60']\n",
    "\n",
    "#### 要舍去的feature 缺失率>0.2 且取值个数超过100的\n",
    "['f114', 'f288', 'f289', 'f115', 'f284', 'f285', 'f286', 'f287', 'f280', 'f281', 'f282', 'f283', 'f43', 'f294', 'f45', 'f44', 'f293', 'f292', 'f291', 'f290', 'f118', 'f119', 'f297', 'f296', 'f116', 'f117', 'f110', 'f111', 'f112', 'f113', 'f56', 'f57', 'f54', 'f55', 'f51', 'f58', 'f295', 'f109', 'f108', 'f107', 'f106', 'f105', 'f104', 'f103', 'f102', 'f23', 'f22', 'f27', 'f130', 'f134', 'f35', 'f121', 'f120', 'f122', 'f146', 'f85', 'f84', 'f86', 'f81', 'f80', 'f83', 'f82', 'f150', 'f151', 'f152', 'f153', 'f154', 'f143', 'f142', 'f147', 'f5', 'f145', 'f144', 'f149', 'f148', 'f69', 'f68', 'f67', 'f66', 'f64', 'f279', 'f278', 'f78', 'f79', 'f74', 'f75', 'f76', 'f77', 'f70', 'f71', 'f72', 'f73']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失率>0.2且取值个数不超过100的feature做ohe\n",
    "- 注意train和test统一处理\n",
    "- 注意nan特殊情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['f125', 'f124', 'f127', 'f126', 'f123', 'f160', 'f129', 'f128', 'f91', 'f140', 'f99', 'f101', 'f100', 'f41', 'f40', 'f42', 'f89', 'f88', 'f47', 'f46', 'f49', 'f48', 'f87', 'f21', 'f20', 'f26', 'f25', 'f24', 'f29', 'f28', 'f63', 'f62', 'f61', 'f65', 'f155', 'f156', 'f157', 'f158', 'f159', 'f92', 'f93', 'f138', 'f139', 'f96', 'f97', 'f94', 'f95', 'f132', 'f133', 'f98', 'f131', 'f136', 'f137', 'f135', 'f52', 'f53', 'f50', 'f90', 'f59', 'f30', 'f31', 'f32', 'f33', 'f34', 'f36', 'f37', 'f38', 'f39', 'f141', 'f60']\n",
    "\n",
    "# 判断每个feature的unique value count，决定是否对该特征one-hot encoding\n",
    "# 这里的unique_count是不包括nan的\n",
    "def unique_count(data):\n",
    "    data_unique_count = pd.Series()\n",
    "    for _ in data.columns:\n",
    "        data_unique_count[_] = data[_].unique().shape[0]\n",
    "    \n",
    "    return data_unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_unique_count = unique_count(data_train_feature)\n",
    "data_test_unique_count = unique_count(data_test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对比train和test中feature取值个数\n",
    "- 取值个数不一样，所以做ohe的时候要统一，否则后期训练预测会有问题！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较feature取值个数是否相等\n",
    "data_train_test_unique_count_categorical = pd.concat([data_train_unique_count[categorical_features], data_test_unique_count[categorical_features]], axis=1)\n",
    "data_train_test_unique_count_categorical.columns = [\"train_unique_count\", \"test_unique_count\"]\n",
    "\n",
    "print list(data_train_test_unique_count_categorical[data_train_test_unique_count_categorical[\"train_unique_count\"] == data_train_test_unique_count_categorical[\"test_unique_count\"]].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于类别型变量train和test取值个数不一样的，统计它们每个取值次数的分布\n",
    "categorical_columns_diff = ['f160', 'f114', 'f288', 'f289', 'f145', 'f115', 'f284', 'f285', 'f286', 'f287', 'f280', 'f281', 'f282', 'f283', 'f41', 'f43', 'f42', 'f45', 'f44', 'f47', 'f46', 'f49', 'f48', 'f111', 'f144', 'f112', 'f113', 'f118', 'f119', 'f297', 'f296', 'f295', 'f290', 'f56', 'f57', 'f54', 'f55', 'f52', 'f53', 'f50', 'f51', 'f58', 'f142', 'f116', 'f109', 'f107', 'f106', 'f105', 'f104', 'f102', 'f23', 'f22', 'f21', 'f20', 'f27', 'f26', 'f25', 'f24', 'f29', 'f28', 'f110', 'f138', 'f139', 'f132', 'f133', 'f130', 'f131', 'f136', 'f137', 'f134', 'f135', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f125', 'f124', 'f126', 'f121', 'f120', 'f123', 'f122', 'f129', 'f128', 'f140', 'f147', 'f5', 'f89', 'f88', 'f117', 'f85', 'f84', 'f87', 'f86', 'f81', 'f80', 'f83', 'f82', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f92', 'f93', 'f90', 'f96', 'f97', 'f94', 'f95', 'f143', 'f146', 'f149', 'f148', 'f141', 'f69', 'f68', 'f66', 'f65', 'f64', 'f78', 'f79', 'f74', 'f75', 'f76', 'f77', 'f71', 'f72', 'f73']\n",
    "def value_count_ratio(feature):\n",
    "    a = (data_train_feature[feature].value_counts(sort=False, dropna=False) / len(data_train_feature[feature])).sort_index()\n",
    "    b = (data_test_feature[feature].value_counts(sort=False, dropna=False) / len(data_test_feature[feature])).sort_index()\n",
    "    c = pd.concat([a, b], axis=1)\n",
    "    print c\n",
    "\n",
    "# 发现大多数取值个数是train比test多，少部分是test比train多\n",
    "# 统一fit，再transform不要把多出来的值归为一类，这样是不加入任何噪声的\n",
    "for feature in categorical_columns_diff:\n",
    "    value_count_ratio(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train和test中的feature统一ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding还是不能加载整个dataframe，太慢，还是单独一个series出来one-hot encoding\n",
    "# 千万注意，na不要当做一个类来看了！！\n",
    "# 取值个数无论是否一样的feature都统一处理\n",
    "# 检查f88特征\n",
    "\n",
    "enc = OneHotEncoder(sparse=False, dtype=np.uint8)\n",
    "def one_hot_encoding(feature, data_train_feature, data_test_feature):\n",
    "    print \"one-hot encoding feature: \", feature\n",
    "    global data_ohe_train\n",
    "    global data_ohe_test\n",
    "    columns = []\n",
    "    # train和test统一处理，按个数多的那个处理！\n",
    "    value_unique_combine = set(data_train_feature[feature].fillna(999999)) | set(data_test_feature[feature].fillna(999999))\n",
    "    unique_count = len(value_unique_combine)\n",
    "    print unique_count\n",
    "    for i in range(unique_count):\n",
    "        columns.append(feature + \"_\" + str(i))\n",
    "    enc.fit(np.array(list(value_unique_combine)).reshape(-1, 1))\n",
    "    \n",
    "    single_ohe_train = pd.DataFrame(enc.transform(data_train_feature[feature].fillna(999999).values.reshape(-1, 1)), columns=columns)\n",
    "    single_ohe_test = pd.DataFrame(enc.transform(data_test_feature[feature].fillna(999999).values.reshape(-1, 1)), columns=columns)\n",
    "    \n",
    "    data_train_feature.drop(labels=feature, axis=1, inplace=True)\n",
    "    data_test_feature.drop(labels=feature, axis=1, inplace=True)\n",
    "\n",
    "    data_ohe_train = pd.concat([data_ohe_train, single_ohe_train], axis=1)\n",
    "    data_ohe_test = pd.concat([data_ohe_test, single_ohe_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_ohe_train = pd.DataFrame()\n",
    "data_ohe_test = pd.DataFrame()\n",
    "\n",
    "# train和test统一处理\n",
    "for feature in categorical_features:\n",
    "    one_hot_encoding(feature, data_train_feature, data_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe_train.to_hdf(\"../data/train_ohe.hdf\", \"train_ohe\")\n",
    "data_ohe_test.to_hdf(\"../data/test_ohe.hdf\", \"test_ohe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ohe特征的方差筛选阈值确认\n",
    "- 确认为0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ohe特征的方差筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 十分注意！np.var对于array会注意到nan，对于series会自动忽略nan\n",
    "\n",
    "def df_var(data):\n",
    "    feature_var = pd.Series()\n",
    "    columns = data.columns\n",
    "    for _ in range(len(columns)):\n",
    "        if _ % 100 == 0:\n",
    "            print _\n",
    "        \n",
    "        feature_var[columns[_]] = np.var(data[columns[_]])\n",
    "#     feature_var.hist(bins=np.arange(0, 1.05, 0.025))\n",
    "    return feature_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe_train = pd.read_hdf(\"../data/train_ohe.hdf\")\n",
    "data_ohe_test = pd.read_hdf(\"../data/test_ohe.hdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ohe_train_var = df_var(data_ohe_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_train_var = df_var(data_ohe_train)\n",
    "larger_var_features = list(ohe_train_var[ohe_train_var > 0.1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe_train = data_ohe_train[larger_var_features]\n",
    "data_ohe_test = data_ohe_test[larger_var_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe_train.to_hdf(\"../data/train_ohe_large_var.hdf\", \"train_ohe\")\n",
    "data_ohe_test.to_hdf(\"../data/test_ohe_large_var.hdf\", \"test_ohe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺失率>0.2 且取值个数超过100的feature舍去\n",
    "- 注意做了ohe的feature也要舍去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意ohe的feature在ohe的时候会被去掉，所以这里再手动去掉一下，没问题！\n",
    "drop_features = set(['f114', 'f288', 'f289', 'f115', 'f284', 'f285', 'f286', 'f287', 'f280', 'f281', 'f282', 'f283', 'f43', 'f294', 'f45', 'f44', 'f293', 'f292', 'f291', 'f290', 'f118', 'f119', 'f297', 'f296', 'f116', 'f117', 'f110', 'f111', 'f112', 'f113', 'f56', 'f57', 'f54', 'f55', 'f51', 'f58', 'f295', 'f109', 'f108', 'f107', 'f106', 'f105', 'f104', 'f103', 'f102', 'f23', 'f22', 'f27', 'f130', 'f134', 'f35', 'f121', 'f120', 'f122', 'f146', 'f85', 'f84', 'f86', 'f81', 'f80', 'f83', 'f82', 'f150', 'f151', 'f152', 'f153', 'f154', 'f143', 'f142', 'f147', 'f5', 'f145', 'f144', 'f149', 'f148', 'f69', 'f68', 'f67', 'f66', 'f64', 'f279', 'f278', 'f78', 'f79', 'f74', 'f75', 'f76', 'f77', 'f70', 'f71', 'f72', 'f73'])\n",
    "ohe_features = set(['f125', 'f124', 'f127', 'f126', 'f123', 'f160', 'f129', 'f128', 'f91', 'f140', 'f99', 'f101', 'f100', 'f41', 'f40', 'f42', 'f89', 'f88', 'f47', 'f46', 'f49', 'f48', 'f87', 'f21', 'f20', 'f26', 'f25', 'f24', 'f29', 'f28', 'f63', 'f62', 'f61', 'f65', 'f155', 'f156', 'f157', 'f158', 'f159', 'f92', 'f93', 'f138', 'f139', 'f96', 'f97', 'f94', 'f95', 'f132', 'f133', 'f98', 'f131', 'f136', 'f137', 'f135', 'f52', 'f53', 'f50', 'f90', 'f59', 'f30', 'f31', 'f32', 'f33', 'f34', 'f36', 'f37', 'f38', 'f39', 'f141', 'f60'])\n",
    "left_features = set(data_train_feature.columns) - drop_features - ohe_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_feature = data_train_feature[list(left_features)]\n",
    "data_test_feature = data_test_feature[list(left_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print data_train_feature.info()\n",
    "print data_test_feature.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 余下的特征，缺失率比较小，用均值填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_fillna_mean(data):\n",
    "    for feature in data.columns:\n",
    "        data[feature] = data[feature].fillna(data[feature].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_feature = pd.concat([data_train_feature, data_ohe_train], axis=1)\n",
    "data_test_feature = pd.concat([data_test_feature, data_ohe_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print data_train_feature.info()\n",
    "print data_test_feature.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fillna_mean(data_train_feature)\n",
    "data_fillna_mean(data_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_feature.to_hdf(\"../data/train_feature_left.hdf\", \"train\")\n",
    "data_test_feature.to_hdf(\"../data/test_feature_left.hdf\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征提取\n",
    "- 在原始数据基础上进行特征扩充：变量主体、度量维度（时间、次数、占比、排名）、时间窗口（1、3、7天）、聚合函数（min, max, avg，total, std, 用来捕捉时间序列上的变化趋势）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征是否存在作为扩充特征，存在为1，不存在为0\n",
    "- 这个只针对有缺失值的特征，没有缺失值的特征扩充这个维度的特征没有意义，所以后期会有方差筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_exist(data):\n",
    "    data_exist = data.notnull().astype(np.uint8)\n",
    "    data_exist.columns = np.array(data_exist.columns) + \"_exist\"\n",
    "    \n",
    "    return data_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exist = feature_exist(data_train_feature)\n",
    "data_exist.to_hdf(\"../data/train_exist.hdf\", \"train\")\n",
    "\n",
    "data_exist = feature_exist(data_test_feature)\n",
    "data_exist.to_hdf(\"../data/test_exist.hdf\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 是否存在特征的方差筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 十分注意！np.var对于array会注意到nan，对于series会自动忽略nan\n",
    "\n",
    "def df_var(data):\n",
    "    feature_var = pd.Series()\n",
    "    columns = data.columns\n",
    "    for _ in range(len(columns)):\n",
    "        if _ % 100 == 0:\n",
    "            print _\n",
    "        \n",
    "        feature_var[columns[_]] = np.var(data[columns[_]])\n",
    "    feature_var.hist(bins=np.arange(0, 1.05, 0.025))\n",
    "    return feature_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exist = pd.read_pickle(\"../data/train_exist\")\n",
    "test_exist = pd.read_pickle(\"../data//test_a_exist\")\n",
    "train_exist_var = df_var(train_exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exist = train_exist[train_exist_var[train_exist_var > 0.1].index]\n",
    "test_exist = test_exist[train_exist_var[train_exist_var > 0.1].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exist.to_hdf(\"../data/train_exist_large_var.hdf\", \"train\")\n",
    "test_exist.to_hdf(\"../data/test_exist_large_var.hdf\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 时间特征提取及筛选\n",
    "1. weekday\n",
    "2. monthday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_date = pd.read_pickle(\"/home/mountain/atec/data/raw_pickle_split/train_date\")\n",
    "data_test_date = pd.read_pickle(\"/home/mountain/atec/data/raw_pickle_split/test_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_weekday(date_int):\n",
    "    a = str(date_int)\n",
    "    day = datetime(year=int(a[:4]), month=int(a[4:6]), day=int(a[6:]))\n",
    "    return day.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthday(date_int):\n",
    "    return date_int % 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse=False, dtype=np.uint8)\n",
    "data_train_weekday = data_train_date.apply(get_weekday)\n",
    "data_train_weekday_ohe = enc.fit_transform(data_train_weekday.values.reshape(-1, 1))\n",
    "\n",
    "enc = OneHotEncoder(sparse=False, dtype=np.uint8)\n",
    "data_train_monthday = data_train_date.apply(get_monthday)\n",
    "data_train_monthday_ohe = enc.fit_transform(data_train_monthday.values.reshape(-1, 1))\n",
    "data_train_date_ohe = pd.DataFrame(np.concatenate((data_train_weekday_ohe, data_train_monthday_ohe), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data_train_date_ohe).to_hdf(\"../data/train_date_ohe.hdf\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse=False, dtype=np.uint8)\n",
    "data_test_weekday = data_test_date.apply(get_weekday)\n",
    "data_test_weekday_ohe = enc.fit_transform(data_test_weekday.values.reshape(-1, 1))\n",
    "\n",
    "enc = OneHotEncoder(sparse=False, dtype=np.uint8)\n",
    "data_test_monthday = data_test_date.apply(get_monthday)\n",
    "data_test_monthday_ohe = enc.fit_transform(data_test_monthday.values.reshape(-1, 1))\n",
    "\n",
    "data_test_date_ohe = pd.DataFrame(np.concatenate((data_test_weekday_ohe, data_test_monthday_ohe), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 十分注意！np.var对于array会注意到nan，对于series会自动忽略nan\n",
    "\n",
    "def df_var(data):\n",
    "    feature_var = pd.Series()\n",
    "    columns = data.columns\n",
    "    for _ in range(len(columns)):\n",
    "#         print _\n",
    "        feature_var[str(columns[_])] = np.var(data[columns[_]])\n",
    "    feature_var.hist()\n",
    "    return feature_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ohe_train_date_var = df_var(data_train_date_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ohe_train_date_var[ohe_train_date_var > 0.1].index\n",
    "t = [int(x) for x in _]\n",
    "data_train_date_ohe = data_train_date_ohe[t]\n",
    "data_test_date_ohe = data_test_date_ohe[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cplumns(data):\n",
    "    data.columns = [\"date_\" + str(_) for _ in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_date_ohe.to_hdf(\"../data/train_date_ohe_large_var.hdf\", \"train\")\n",
    "data_test_date_ohe.to_hdf(\"../data/test_date_ohe_large_var.hdf\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取一段时间内某个特征的统计变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只要改data_train就可以了！\n",
    "data_train = pd.read_pickle(\"../data/raw_pickle/test_a\")\n",
    "\n",
    "data_train.drop(labels=[\"id\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一天的feature的min, max, avg, std, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_min = data_train.groupby(\"date\").min()\n",
    "one_day_max = data_train.groupby(\"date\").max()\n",
    "one_day_mean = data_train.groupby(\"date\").mean()\n",
    "one_day_std = data_train.groupby(\"date\").std()\n",
    "one_day_sum = data_train.groupby(\"date\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_min.columns = one_day_min.columns + \"_min_one\"\n",
    "one_day_max.columns = one_day_max.columns + \"_max_one\"\n",
    "one_day_mean.columns = one_day_mean.columns + \"_mean_one\"\n",
    "one_day_std.columns = one_day_std.columns + \"_std_one\"\n",
    "one_day_sum.columns = one_day_sum.columns + \"_sum_one\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三天的feature的min, max, avg, std, total\n",
    "- 对于头部数据，只取当天的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = np.sort(data_train[\"date\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_day_min = pd.DataFrame(columns=data_train.columns)\n",
    "three_day_max = pd.DataFrame(columns=data_train.columns)\n",
    "three_day_mean = pd.DataFrame(columns=data_train.columns)\n",
    "three_day_std = pd.DataFrame(columns=data_train.columns)\n",
    "three_day_sum = pd.DataFrame(columns=data_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d in date:\n",
    "    print d\n",
    "    tdd = data_train[((int(d) - data_train[\"date\"].apply(int)) <= 2) & ((int(d) - data_train[\"date\"].apply(int)) >= 0)]\n",
    "    tdd_min = tdd.min()\n",
    "    tdd_max = tdd.max()\n",
    "    tdd_mean = tdd.mean()\n",
    "    tdd_std = tdd.std()\n",
    "    tdd_sum = tdd.sum()\n",
    "    \n",
    "    tdd_min[\"date\"] = d\n",
    "    tdd_max[\"date\"] = d\n",
    "    tdd_mean[\"date\"] = d\n",
    "    tdd_std[\"date\"] = d\n",
    "    tdd_sum[\"date\"] = d\n",
    "    \n",
    "    three_day_min = three_day_min.append(tdd_min, ignore_index=True)  \n",
    "    three_day_max = three_day_max.append(tdd_max, ignore_index=True)  \n",
    "    three_day_mean = three_day_mean.append(tdd_mean, ignore_index=True)  \n",
    "    three_day_std = three_day_std.append(tdd_std, ignore_index=True)  \n",
    "    three_day_sum = three_day_sum.append(tdd_sum, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "three_day_min.set_index(\"date\", inplace=True)\n",
    "three_day_max.set_index(\"date\", inplace=True)\n",
    "three_day_mean.set_index(\"date\", inplace=True)\n",
    "three_day_std.set_index(\"date\", inplace=True)\n",
    "three_day_sum.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_day_min.columns = three_day_min.columns + \"_min_three\"\n",
    "three_day_max.columns = three_day_max.columns + \"_max_three\"\n",
    "three_day_mean.columns = three_day_mean.columns + \"_mean_three\"\n",
    "three_day_std.columns = three_day_std.columns + \"_std_three\"\n",
    "three_day_sum.columns = three_day_sum.columns + \"_sum_three\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 七天的feature的min, max, avg, std, total\n",
    "- 对于头部数据，只取当天的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = np.sort(data_train[\"date\"].unique())\n",
    "\n",
    "seven_day_min = pd.DataFrame(columns=data_train.columns)\n",
    "seven_day_max = pd.DataFrame(columns=data_train.columns)\n",
    "seven_day_mean = pd.DataFrame(columns=data_train.columns)\n",
    "seven_day_std = pd.DataFrame(columns=data_train.columns)\n",
    "seven_day_sum = pd.DataFrame(columns=data_train.columns)\n",
    "\n",
    "for d in date:\n",
    "    print d\n",
    "    tdd = data_train[((int(d) - data_train[\"date\"].apply(int)) <= 6) & ((int(d) - data_train[\"date\"].apply(int)) >= 0)]\n",
    "    tdd_min = tdd.min()\n",
    "    tdd_max = tdd.max()\n",
    "    tdd_mean = tdd.mean()\n",
    "    tdd_std = tdd.std()\n",
    "    tdd_sum = tdd.sum()\n",
    "    \n",
    "    tdd_min[\"date\"] = d\n",
    "    tdd_max[\"date\"] = d\n",
    "    tdd_mean[\"date\"] = d\n",
    "    tdd_std[\"date\"] = d\n",
    "    tdd_sum[\"date\"] = d\n",
    "    \n",
    "    seven_day_min = seven_day_min.append(tdd_min, ignore_index=True)  \n",
    "    seven_day_max = seven_day_max.append(tdd_max, ignore_index=True)  \n",
    "    seven_day_mean = seven_day_mean.append(tdd_mean, ignore_index=True)  \n",
    "    seven_day_std = seven_day_std.append(tdd_std, ignore_index=True)  \n",
    "    seven_day_sum = seven_day_sum.append(tdd_sum, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_day_min.set_index(\"date\", inplace=True)\n",
    "seven_day_max.set_index(\"date\", inplace=True)\n",
    "seven_day_mean.set_index(\"date\", inplace=True)\n",
    "seven_day_std.set_index(\"date\", inplace=True)\n",
    "seven_day_sum.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_day_min.columns = seven_day_min.columns + \"_min_seven\"\n",
    "seven_day_max.columns = seven_day_max.columns + \"_max_seven\"\n",
    "seven_day_mean.columns = seven_day_mean.columns + \"_mean_seven\"\n",
    "seven_day_std.columns = seven_day_std.columns + \"_std_seven\"\n",
    "seven_day_sum.columns = seven_day_sum.columns + \"_sum_seven\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = pd.concat([one_day_min, one_day_max, one_day_mean, one_day_std, one_day_sum], axis=1)\n",
    "three_day = pd.concat([three_day_min, three_day_max, three_day_mean, three_day_std, three_day_sum], axis=1)\n",
    "seven_day = pd.concat([seven_day_min, seven_day_max, seven_day_mean, seven_day_std, seven_day_sum], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_type(data):\n",
    "    data.index = data.index.astype(np.int32)\n",
    "    return None\n",
    "\n",
    "def index_reset(data):\n",
    "    data.reset_index(level=\"date\", inplace=True)\n",
    "\n",
    "# float, int转换数据格式，降低内存\n",
    "def dtype_descend(data):\n",
    "    data_float = data.select_dtypes(include=['float'])\n",
    "    data_converted_float = data_float.apply(pd.to_numeric, downcast='float')\n",
    "    for column in data_converted_float.columns:\n",
    "#         print \"float \", column\n",
    "        data[column] = data_converted_float[column]\n",
    "    \n",
    "    data_int = data.select_dtypes(include=['int'])\n",
    "    data_converted_int = data_int.apply(pd.to_numeric, downcast='unsigned')\n",
    "    for column in data_converted_int.columns:\n",
    "#         print \"int \", column\n",
    "        data[column] = data_converted_int[column]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_type(one_day)\n",
    "# index_type(three_day)\n",
    "# index_type(seven_day)\n",
    "\n",
    "# index_reset(one_day)\n",
    "# index_reset(three_day)\n",
    "# index_reset(seven_day)\n",
    "\n",
    "# one_day = dtype_descend(one_day)\n",
    "# three_day = dtype_descend(three_day)\n",
    "# seven_day = dtype_descend(seven_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print one_day.info()\n",
    "print three_day.info()\n",
    "print seven_day.info()\n",
    "\n",
    "one_day.to_pickle(\"../data/feature_extract/one_day\")\n",
    "three_day.to_pickle(\"../data/feature_extract/three_day\")\n",
    "seven_day.to_pickle(\"../data/feature_extract/seven_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print one_day.info()\n",
    "print three_day.info()\n",
    "print seven_day.info()\n",
    "\n",
    "one_day.to_pickle(\"../data/feature_extract/one_day_test\")\n",
    "three_day.to_pickle(\"../data/feature_extract/three_day_test\")\n",
    "seven_day.to_pickle(\"../data/feature_extract/seven_day_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取出的特征拼接及筛选（方差及皮尔逊）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据特征和标签的皮尔逊相关系数，来进行特征筛选，选前200个\n",
    "# 要自己写底层的东西\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def corr_filter(data, label, k):\n",
    "    print \"corr_filter\"\n",
    "    columns = data.columns\n",
    "    pearson_series = pd.Series()\n",
    "    for _ in range(len(columns)):\n",
    "        if _ % 100 == 0:\n",
    "            print _\n",
    "        pearson_series[columns[_]] = pearsonr(data[columns[_]], label.values.reshape(-1,))[0]\n",
    "    pearson_series.hist()\n",
    "    \n",
    "    new_columns = list(abs(pearson_series).sort_values(ascending=False).index)[:k]\n",
    "    data_new = pd.DataFrame()\n",
    "    for _ in new_columns:\n",
    "#         print _\n",
    "        data_new[_] = data[_]\n",
    "    \n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_filter(data):\n",
    "    print \"var_filter\"\n",
    "    feature_var = pd.Series()\n",
    "    columns = data.columns\n",
    "    for _ in range(len(columns)):\n",
    "        if _ % 100 == 0:\n",
    "            print _\n",
    "        \n",
    "        feature_var[columns[_]] = np.var(data[columns[_]])\n",
    "#     feature_var.hist()\n",
    "    \n",
    "    data_new = pd.DataFrame()\n",
    "    new_columns = list(feature_var[feature_var > 0.1].index)\n",
    "    for _ in new_columns:\n",
    "#         print _\n",
    "        data_new[_] = data[_]\n",
    "    \n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day = pd.read_hdf(\"../data/one_day.hdf\")\n",
    "three_day = pd.read_hdf(\"../data/three_day.hdf\")\n",
    "seven_day = pd.read_hdf(\"../data/seven_day.hdf\")\n",
    "data_train_date = pd.DataFrame(pd.read_pickle(\"../data/raw_pickle_split/train_date\"), columns=[\"date\"])\n",
    "data_train_label = pd.DataFrame(pd.read_pickle(\"../data/raw_pickle_split/train_label\"), columns=[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意！dataframe允许重复列的存在，在pd.merge时如果有重复列会自动添加suffix后缀！\n",
    "comb = pd.merge(data_train_date, one_day, how=\"left\", on=\"date\", sort=False)\n",
    "comb = corr_filter(comb, data_train_label, 200)\n",
    "comb = var_filter(comb)\n",
    "comb[\"date\"] = data_train_date[\"date\"]\n",
    "\n",
    "comb = pd.merge(comb, three_day, how=\"left\", on=\"date\", sort=False)\n",
    "comb = corr_filter(comb, data_train_label, 200)\n",
    "comb = var_filter(comb)\n",
    "comb[\"date\"] = data_train_date[\"date\"]\n",
    "\n",
    "comb = pd.merge(comb, seven_day, how=\"left\", on=\"date\", sort=False)\n",
    "comb = corr_filter(comb, data_train_label, 200)\n",
    "comb = var_filter(comb)\n",
    "comb[\"date\"] = data_train_date[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb.to_hdf(\"../data/train_time_feature_sv\", \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test里面的特征筛选要和train里面是一致的！\n",
    "one_day_test = pd.read_hdf(\"../data/one_day_test.hdf\")\n",
    "three_day_test = pd.read_hdf(\"../data/three_day_test.hdf\")\n",
    "seven_day_test = pd.read_hdf(\"../data/seven_day_test.hdf\")\n",
    "data_test_date = pd.DataFrame(pd.read_pickle(\"../data/raw_pickle_split/test_date\"), columns=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comb = pd.read_hdf(\"../data/train_time_feature_sv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_test = one_day_test[list(set(one_day_test.columns) & set(train_comb.columns))]\n",
    "three_day_test = three_day_test[list(set(three_day_test.columns) & set(train_comb.columns))]\n",
    "seven_day_test = seven_day_test[list(set(seven_day_test.columns) & set(train_comb.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comb = pd.merge(data_test_date, one_day_test, how=\"left\", on=\"date\", sort=False)\n",
    "comb = pd.merge(comb, three_day_test, how=\"left\", on=\"date\", sort=False)\n",
    "comb = pd.merge(comb, seven_day_test, how=\"left\", on=\"date\", sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb.to_hdf(\"../data/test_time_feature_sv\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取一段时间内某个特征的缺失率\n",
    "- 统计某个特征某一天的缺失率，反应缺失值的分布变化\n",
    "- 最后发现缺失率特征方差都特别小，没有意义！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_pickle(\"/home/mountain/atec/data/raw_pickle/train\")\n",
    "data_test = pd.read_pickle(\"/home/mountain/atec/data/raw_pickle/test_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nan_ratio(data):\n",
    "    nan_ratio = 1.0 * data.isnull().sum() / data.shape[0]\n",
    "    \n",
    "    return nan_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_nan = data_train.groupby(\"date\").apply(get_nan_ratio)\n",
    "one_day_nan.drop(labels=[\"id\", \"label\", \"date\"], inplace=True, axis=1)\n",
    "\n",
    "one_day_nan.columns = one_day_nan.columns + \"_daily_nan\"\n",
    "\n",
    "one_day_nan.to_pickle(\"/home/mountain/atec/data/feature_extract/one_day_nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_nan_test = data_test.groupby(\"date\").apply(get_nan_ratio)\n",
    "one_day_nan_test.drop(labels=[\"id\", \"date\"], inplace=True, axis=1)\n",
    "one_day_nan_test.columns = one_day_nan_test.columns + \"_daily_nan\"\n",
    "one_day_nan_test.to_pickle(\"/home/mountain/atec/data/feature_extract/one_day_nan_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_day_nan = pd.read_pickle(\"/home/mountain/atec/data/feature_extract/one_day_nan\")\n",
    "data_train_date = pd.DataFrame(pd.read_pickle(\"../data/raw_pickle_split/train_date\"), columns=[\"date\"])\n",
    "data_train_label = pd.DataFrame(pd.read_pickle(\"../data/raw_pickle_split/train_label\"), columns=[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_type(one_day_nan)\n",
    "index_reset(one_day_nan)\n",
    "one_day_nan = dtype_descend(one_day_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "one_day_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = pd.merge(data_train_date, one_day_nan, how=\"left\", on=\"date\", sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = corr_filter(comb, data_train_label, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 注意！dataframe允许重复列的存在，在pd.merge时如果有重复列会自动添加suffix后缀！\n",
    "comb = var_filter(comb)\n",
    "comb[\"date\"] = data_train_date[\"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rank排名\n",
    "- 统计所有时段的某个特征某个取值对应的bad_rate（这个bad_rate仅仅包括1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_feature = pd.read_pickle(\"/home/mountain/atec/data/raw_pickle_split/train_feature\")\n",
    "data_train_label = pd.read_pickle(\"/home/mountain/atec/data/raw_pickle_split/train_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(data_train_label[data_train_feature[\"f50\"][data_train_feature[\"f50\"] == 1].index] == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(data_train_label[data_train_feature[\"f50\"][data_train_feature[\"f50\"] == 1].index] == -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_bad_rate(series):\n",
    "    for value in series.unique():\n",
    "        value_index = series[series == value].index\n",
    "        data_train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单调性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature标准化，标准化完注意dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scale(data_train_feature, data_test_feature):\n",
    "    for feature in data_train_feature.columns:\n",
    "        print feature\n",
    "        scaler = MinMaxScaler()\n",
    "        train_scaler = scaler.fit(data_train_feature[feature].values.reshape(-1, 1))\n",
    "        train_transform = scaler.transform(data_train_feature[feature].values.reshape(-1, 1))\n",
    "        test_transform = scaler.transform(data_test_feature[feature].values.reshape(-1, 1))\n",
    "        data_train_feature[feature] = pd.DataFrame(train_transform, dtype=np.float32)\n",
    "        data_test_feature[feature] = pd.DataFrame(test_transform, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征筛选\n",
    "- 方差筛选\n",
    "- 皮尔逊筛选\n",
    "- 先拼接起来，有train_feature_left, train_ohe_large_var, train_exist_large_var, train_date_ohe_large_var, train_time_feature_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整体统一筛选\n",
    "# coding:utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import gc\n",
    "\n",
    "# 注意观察内存问题！！\n",
    "\n",
    "# 方差筛选过程中，去除的比例\n",
    "drop_prop = 0.8\n",
    "\n",
    "# 皮尔逊筛选过程中，留下的个数\n",
    "select_num = 1000\n",
    "\n",
    "def df_var(data):\n",
    "\tprint \"df_var\"\n",
    "\tfeature_var = pd.Series()\n",
    "\tcolumns = data.columns\n",
    "\tfor _ in range(len(columns)):\n",
    "\t\tif _ % 100 == 0:\n",
    "\t\t\tprint _\n",
    "\t\tfeature_var[columns[_]] = np.var(data[columns[_]])\n",
    "\tdel columns\n",
    "\treturn feature_var\n",
    "\n",
    "\n",
    "# 对于0-1特征(ohe, exist, 时间特征)，统一进行方差筛选\n",
    "print \"ohe\"\n",
    "data_train_ohe = pd.read_hdf(\"../data/train_ohe.hdf\")\n",
    "data_train_ohe_var = df_var(data_train_ohe)\n",
    "del data_train_ohe\n",
    "\n",
    "print \"exist\"\n",
    "data_train_exist = pd.read_hdf(\"../data/train_exist.hdf\")\n",
    "data_train_exist_var = df_var(data_train_exist)\n",
    "del data_train_exist\n",
    "\n",
    "print \"date\"\n",
    "data_train_date_ohe = pd.read_hdf(\"../data/train_date_ohe.hdf\")\n",
    "data_train_date_ohe_var = df_var(data_train_date_ohe)\n",
    "del data_train_date_ohe\n",
    "\n",
    "data_train_ohe_exist_date_var = pd.concat([data_train_ohe_var, data_train_exist_var, data_train_date_ohe_var])\n",
    "del data_train_ohe_var, data_train_exist_var, data_train_date_ohe_var\n",
    "\n",
    "data_train_ohe_exist_date_var_sort = data_train_ohe_exist_date_var.sort_values()\n",
    "del data_train_ohe_exist_date_var\n",
    "\n",
    "# 取20%\n",
    "# 千万注意，这里默认是升序排列\n",
    "threshold = int(len(data_train_ohe_exist_date_var_sort) * drop_prop)\n",
    "large_var_index = data_train_ohe_exist_date_var_sort.index[threshold:]\n",
    "del data_train_ohe_exist_date_var_sort, threshold\n",
    "\n",
    "data_train_ohe = pd.read_hdf(\"../data/train_ohe.hdf\")\n",
    "data_train_exist = pd.read_hdf(\"../data/train_exist.hdf\")\n",
    "data_train_date_ohe = pd.read_hdf(\"../data/train_date_ohe.hdf\")\n",
    "\n",
    "# 因为这里都是uint8，所以很小，不用分治拼接！\n",
    "data_train_ohe_exist_date_large_var = pd.concat([data_train_ohe, data_train_exist, data_train_date_ohe], axis=1)[large_var_index]\n",
    "del data_train_ohe, data_train_exist, data_train_date_ohe\n",
    "data_train_ohe_exist_date_large_var.to_hdf(\"../data/train_ohe_exist_date_large_var.hdf\", \"train\")\n",
    "del data_train_ohe_exist_date_large_var\n",
    "\n",
    "# test\n",
    "data_test_ohe = pd.read_hdf(\"../data/test_ohe.hdf\")\n",
    "data_test_exist = pd.read_hdf(\"../data/test_exist.hdf\")\n",
    "data_test_date_ohe = pd.read_hdf(\"../data/test_date_ohe.hdf\")\n",
    "data_test_ohe_exist_date_large_var = pd.concat([data_test_ohe, data_test_exist, data_test_date_ohe], axis=1)[large_var_index]\n",
    "del data_test_ohe, data_test_exist, data_test_date_ohe\n",
    "data_test_ohe_exist_date_large_var.to_hdf(\"../data/test_ohe_exist_date_large_var.hdf\", \"test\")\n",
    "del data_test_ohe_exist_date_large_var\n",
    "\n",
    "del large_var_index\n",
    "\n",
    "# 手动回收一下垃圾内存\n",
    "gc.collect()\n",
    "\n",
    "# 对于一段时间内提取出来的特征（一段时间内的统计变量+缺失率），因为实在太大了，所以做皮尔逊筛选+方差筛选\n",
    "\n",
    "# 根据特征和标签的皮尔逊相关系数，来进行特征筛选，选前500个\n",
    "# 要自己写底层的东西\n",
    "def corr_filter(data, label, k):\n",
    "\tprint \"corr_filter\"\n",
    "\tcolumns = data.columns\n",
    "\tpearson_series = pd.Series()\n",
    "\tfor _ in range(len(columns)):\n",
    "\t\tif _ % 100 == 0:\n",
    "\t\t\tprint _\n",
    "\t\tpearson_series[columns[_]] = pearsonr(data[columns[_]], label.values.reshape(-1, ))[0]\n",
    "\t# pearson_series.hist()\n",
    "\n",
    "\tnew_columns = list(abs(pearson_series).sort_values(ascending=False).index)[:k]\n",
    "\n",
    "\tdel columns, pearson_series, data, label, k\n",
    "\n",
    "\treturn new_columns\n",
    "\n",
    "one_day = pd.read_hdf(\"../data/one_day.hdf\")\n",
    "three_day = pd.read_hdf(\"../data/three_day.hdf\")\n",
    "seven_day = pd.read_hdf(\"../data/seven_day.hdf\")\n",
    "data_train_date = pd.DataFrame(pd.read_pickle(\"../data/raw_pickle_split/train_date\"), columns=[\"date\"])\n",
    "data_train_label = pd.DataFrame(pd.read_pickle(\"../data/raw_pickle_split/train_label\"), columns=[\"label\"])\n",
    "\n",
    "# 注意！dataframe允许重复列的存在，在pd.merge时如果有重复列会自动添加suffix后缀！\n",
    "print \"one_day\"\n",
    "comb1 = pd.merge(data_train_date, one_day, how=\"left\", on=\"date\", sort=False)\n",
    "comb2_columns = corr_filter(comb1, data_train_label, select_num)\n",
    "del comb1\n",
    "if \"date\" not in comb2_columns:\n",
    "\tcomb2_columns.append(\"date\")\n",
    "one_day_large_corr = one_day[comb2_columns]\n",
    "del one_day\n",
    "comb2 = pd.merge(data_train_date, one_day_large_corr, how=\"left\", on=\"date\", sort=False)\n",
    "del one_day_large_corr\n",
    "comb2.to_hdf(\"../data/comb2.hdf\", \"train\")\n",
    "del comb2\n",
    "gc.collect()\n",
    "\n",
    "print \"three_day\"\n",
    "comb3 = pd.merge(data_train_date, three_day, how=\"left\", on=\"date\", sort=False)\n",
    "comb4_columns = corr_filter(comb3, data_train_label, select_num)\n",
    "del comb3\n",
    "if \"date\" not in comb4_columns:\n",
    "\tcomb4_columns.append(\"date\")\n",
    "three_day_large_corr = three_day[comb4_columns]\n",
    "del three_day\n",
    "comb4 = pd.merge(data_train_date, three_day_large_corr, how=\"left\", on=\"date\", sort=False)\n",
    "del three_day_large_corr\n",
    "comb4.to_hdf(\"../data/comb4.hdf\", \"train\")\n",
    "del comb4\n",
    "gc.collect()\n",
    "\n",
    "print \"seven_day\"\n",
    "comb5 = pd.merge(data_train_date, seven_day, how=\"left\", on=\"date\", sort=False)\n",
    "comb6_columns = corr_filter(comb5, data_train_label, select_num)\n",
    "del comb5\n",
    "if \"date\" not in comb6_columns:\n",
    "\tcomb6_columns.append(\"date\")\n",
    "seven_day_large_corr = seven_day[comb6_columns]\n",
    "del seven_day\n",
    "comb6 = pd.merge(data_train_date, seven_day_large_corr, how=\"left\", on=\"date\", sort=False)\n",
    "del seven_day_large_corr\n",
    "comb6.to_hdf(\"../data/comb6.hdf\", \"train\")\n",
    "del comb6\n",
    "gc.collect()\n",
    "\n",
    "del select_num, data_train_date, data_train_label\n",
    "\n",
    "# 分别拼接，分治思维，不断优化\n",
    "comb2 = pd.read_hdf(\"../data/comb2.hdf\")\n",
    "comb2_var = df_var(comb2)\n",
    "del comb2\n",
    "gc.collect()\n",
    "\n",
    "comb4 = pd.read_hdf(\"../data/comb4.hdf\")\n",
    "comb4_var = df_var(comb4)\n",
    "del comb4\n",
    "gc.collect()\n",
    "\n",
    "comb6 = pd.read_hdf(\"../data/comb6.hdf\")\n",
    "comb6_var = df_var(comb6)\n",
    "del comb6\n",
    "gc.collect()\n",
    "\n",
    "# 这里多留一点，3000 * 0.2 = 600\n",
    "comb_var = pd.concat([comb2_var, comb4_var, comb6_var])\n",
    "del comb2_var, comb4_var, comb6_var\n",
    "threshold = int(len(comb_var) * drop_prop)\n",
    "\n",
    "comb_var_sort = comb_var.sort_values()\n",
    "del comb_var\n",
    "large_index = comb_var_sort.index[threshold:]\n",
    "del comb_var_sort, threshold\n",
    "gc.collect()\n",
    "\n",
    "# 把large_index保存下来，然后一个一个拼接，这样快！\n",
    "_ = np.array(large_index)\n",
    "np.save(\"../data/large_index.npy\", _)\n",
    "del large_index, _\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# train根据large_index拼接\n",
    "one_day = pd.read_hdf(\"../data/one_day.hdf\")\n",
    "three_day = pd.read_hdf(\"../data/three_day.hdf\")\n",
    "seven_day = pd.read_hdf(\"../data/seven_day.hdf\")\n",
    "data_date = pd.DataFrame(pd.read_pickle(\"../data/raw_pickle_split/train_date\"), columns=[\"date\"])\n",
    "\n",
    "large_index = np.load(\"../data/large_index.npy\")\n",
    "one_day_large_var = one_day[list(set(one_day.columns) & set(large_index)) + [\"date\"]]\n",
    "del one_day\n",
    "three_day_large_var = three_day[list(set(three_day.columns) & set(large_index)) + [\"date\"]]\n",
    "del three_day\n",
    "seven_day_large_var = seven_day[list(set(seven_day.columns) & set(large_index)) + [\"date\"]]\n",
    "del seven_day\n",
    "\n",
    "print \"one three seven day train\"\n",
    "comb1 = pd.merge(data_date, one_day_large_var, how=\"left\", on=\"date\", sort=False)\n",
    "del one_day_large_var, data_date\n",
    "comb2 = pd.merge(comb1, three_day_large_var, how=\"left\", on=\"date\", sort=False)\n",
    "del comb1, three_day_large_var\n",
    "comb3 = pd.merge(comb2, seven_day_large_var, how=\"left\", on=\"date\", sort=False)\n",
    "del comb2, seven_day_large_var\n",
    "\n",
    "comb3.drop(labels=\"date\", inplace=True, axis=1)\n",
    "comb3.to_hdf(\"../data/train_time_feature_sv.hdf\", \"train\")\n",
    "del comb3, large_index\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# test\n",
    "# test里面的特征筛选要和train里面是一致的！都是根据large_index来！！\n",
    "one_day_test = pd.read_hdf(\"../data/one_day_test.hdf\")\n",
    "three_day_test = pd.read_hdf(\"../data/three_day_test.hdf\")\n",
    "seven_day_test = pd.read_hdf(\"../data/seven_day_test.hdf\")\n",
    "data_test_date = pd.DataFrame(pd.read_pickle(\"../data/raw_pickle_split/test_date\"), columns=[\"date\"])\n",
    "\n",
    "large_index = np.load(\"../data/large_index.npy\")\n",
    "one_day_test_large_var = one_day_test[list(set(one_day_test.columns) & set(large_index)) + [\"date\"]]\n",
    "del one_day_test\n",
    "three_day_test_large_var = three_day_test[list(set(three_day_test.columns) & set(large_index)) + [\"date\"]]\n",
    "del three_day_test\n",
    "seven_day_test_large_var = seven_day_test[list(set(seven_day_test.columns) & set(large_index)) + [\"date\"]]\n",
    "del seven_day_test\n",
    "\n",
    "print \"one three seven day test\"\n",
    "comb1 = pd.merge(data_test_date, one_day_test_large_var, how=\"left\", on=\"date\", sort=False)\n",
    "del one_day_test_large_var, data_test_date\n",
    "comb2 = pd.merge(comb1, three_day_test_large_var, how=\"left\", on=\"date\", sort=False)\n",
    "del comb1, three_day_test_large_var\n",
    "comb3 = pd.merge(comb2, seven_day_test_large_var, how=\"left\", on=\"date\", sort=False)\n",
    "del comb2, seven_day_test_large_var\n",
    "\n",
    "comb3.drop(labels=\"date\", inplace=True, axis=1)\n",
    "comb3.to_hdf(\"../data/test_time_feature_sv.hdf\", \"test\")\n",
    "del comb3, large_index\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# 对于一段时间内的缺失率特征，单独来看\n",
    "print \"one day nan\"\n",
    "one_day_nan = pd.read_hdf(\"../data/one_day_nan.hdf\")\n",
    "data_date = pd.DataFrame(pd.read_pickle(\"../data/raw_pickle_split/train_date\"), columns=[\"date\"])\n",
    "comb = pd.merge(data_date, one_day_nan, how=\"left\", on=\"date\", sort=False)\n",
    "del data_date, one_day_nan\n",
    "\n",
    "comb.drop(labels=\"date\", inplace=True, axis=1)\n",
    "\n",
    "comb_var = df_var(comb)\n",
    "threshold = int(len(comb_var) * drop_prop)\n",
    "comb_var_sort = comb_var.sort_values()\n",
    "del comb_var\n",
    "\n",
    "large_index = comb_var_sort.index[threshold:]\n",
    "del threshold, comb_var_sort\n",
    "\n",
    "comb_large_var = comb[large_index]\n",
    "del comb\n",
    "\n",
    "comb_large_var.to_hdf(\"../data/train_time_nan.hdf\", \"train\")\n",
    "del comb_large_var\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# test\n",
    "print \"one day nan test\"\n",
    "one_day_nan_test = pd.read_hdf(\"../data/one_day_nan_test.hdf\")\n",
    "data_test_date = pd.DataFrame(pd.read_pickle(\"../data/raw_pickle_split/test_date\"), columns=[\"date\"])\n",
    "comb = pd.merge(data_test_date, one_day_nan_test, how=\"left\", on=\"date\", sort=False)\n",
    "del one_day_nan_test, data_test_date\n",
    "\n",
    "comb_large_var = comb[large_index]\n",
    "del large_index, comb\n",
    "\n",
    "comb_large_var.to_hdf(\"../data/test_time_nan.hdf\", \"test\")\n",
    "del comb_large_var\n",
    "gc.collect()\n",
    "\n",
    "# 几个特征拼接起来，有train_feature_left, train_ohe_exist_large_var, train_time_feature_sv, train_time_nan\n",
    "print \"concat train\"\n",
    "data_train_feature_left = pd.read_hdf(\"../data/train_feature_left.hdf\")\n",
    "data_train_ohe_exist_date_large_var = pd.read_hdf(\"../data/train_ohe_exist_date_large_var.hdf\")\n",
    "data_train_time_feature_sv = pd.read_hdf(\"../data/train_time_feature_sv.hdf\")\n",
    "data_train_time_nan = pd.read_hdf(\"../data/train_time_nan.hdf\")\n",
    "\n",
    "data_train_feature_all = pd.concat([data_train_feature_left, data_train_ohe_exist_date_large_var, data_train_time_feature_sv, data_train_time_nan], axis=1)\n",
    "del data_train_feature_left, data_train_ohe_exist_date_large_var, data_train_time_feature_sv, data_train_time_nan\n",
    "gc.collect()\n",
    "\n",
    "data_train_feature_all.to_hdf(\"../data/train_feature_all.hdf\", \"train\")\n",
    "del data_train_feature_all\n",
    "gc.collect()\n",
    "\n",
    "# test\n",
    "print \"concat test\"\n",
    "data_test_feature_left = pd.read_hdf(\"../data/test_feature_left.hdf\")\n",
    "data_test_ohe_exist_date_large_var = pd.read_hdf(\"../data/test_ohe_exist_date_large_var.hdf\")\n",
    "data_test_time_feature_sv = pd.read_hdf(\"../data/test_time_feature_sv.hdf\")\n",
    "data_test_time_nan = pd.read_hdf(\"../data/test_time_nan.hdf\")\n",
    "\n",
    "data_test_feature_all = pd.concat([data_test_feature_left, data_test_ohe_exist_date_large_var, data_test_time_feature_sv, data_test_time_nan], axis=1)\n",
    "del data_test_feature_left, data_test_ohe_exist_date_large_var, data_test_time_feature_sv, data_test_time_nan\n",
    "gc.collect()\n",
    "\n",
    "data_test_feature_all.to_hdf(\"../data/test_feature_all.hdf\", \"test\")\n",
    "del data_test_feature_all\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# 对于高度相关的特征进行筛选，或者利用树模型筛选特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据特征和标签的皮尔逊相关系数，来进行特征筛选，选前200个\n",
    "# 要自己写底层的东西\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def corr_filter(data, label, k):\n",
    "    print \"corr_filter\"\n",
    "    columns = data.columns\n",
    "    pearson_series = pd.Series()\n",
    "    for _ in range(len(columns)):\n",
    "        if _ % 100 == 0:\n",
    "            print _\n",
    "        pearson_series[columns[_]] = pearsonr(data[columns[_]], label.values.reshape(-1,))[0]\n",
    "    \n",
    "    new_columns = list(abs(pearson_series).sort_values(ascending=False).index)[:k]\n",
    "    data_new = pd.DataFrame()\n",
    "    for _ in new_columns:\n",
    "#         print _\n",
    "        data_new[_] = data[_]\n",
    "    \n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_filter(data):\n",
    "    print \"var_filter\"\n",
    "    feature_var = pd.Series()\n",
    "    columns = data.columns\n",
    "    for _ in range(len(columns)):\n",
    "        if _ % 100 == 0:\n",
    "            print _\n",
    "        \n",
    "        feature_var[columns[_]] = np.var(data[columns[_]])\n",
    "#     feature_var.hist()\n",
    "    \n",
    "    data_new = pd.DataFrame()\n",
    "    new_columns = list(feature_var[feature_var > 0.1].index)\n",
    "    for _ in new_columns:\n",
    "#         print _\n",
    "        data_new[_] = data[_]\n",
    "    \n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相似度计算\n",
    "- 计算欧氏距离\n",
    "- 用rbf作为相似度来衡量\n",
    "- 缺失值暂时使用mean来填充\n",
    "- feature做标准化，样本不要做归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用rbf计算-1样本和1样本的相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 见rbf_sim_tf.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pos_1 = data_train[data_train[\"label\"] == 1].drop(labels=[\"id\", \"label\", \"date\"], axis=1)\n",
    "data_train_neg_1 = data_train[data_train[\"label\"] == -1].drop(labels=[\"id\", \"label\", \"date\"], axis=1)\n",
    "\n",
    "data_train_pos_1.to_pickle(\"../data/raw_pickle_split/train_feature_pos_1\")\n",
    "data_train_neg_1.to_pickle(\"../data/raw_pickle_split/train_feature_neg_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算欧氏距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正常的sklearn, scipy，自己写遍历的计算，太慢\n",
    "- 见euc_dist_tradition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf自己写一个\n",
    "- 见euc_dist_tf_split.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "- -1样本，10%以下的不取，其他的90%当做1\n",
    "- 样本160000：800000\n",
    "- XGB\n",
    "- GBDT+LR，stacking\n",
    "- DNN\n",
    "- boosting feature importance\n",
    "- 注意验证是否错行！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "# 整体统一划分\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', filename='train.log', level=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "# 添加-1样本，每次添加舍弃-1中的drop_num个\n",
    "def add_neg_data(drop_num):\n",
    "    logging.info(\"add_neg_data\")\n",
    "    data_train_label = pd.read_pickle(\"../data/raw_pickle_split/train_label\")\n",
    "    rbf_sim = pd.read_pickle(\"../data/rbf_sim\")\n",
    "    data_train_label[rbf_sim[\"neg_index\"][drop_num:]] = 1\n",
    "    del rbf_sim\n",
    "\n",
    "    data_train_label_no_neg = data_train_label[data_train_label != -1]\n",
    "    del data_train_label\n",
    "\n",
    "    data_train_feature_all = pd.read_hdf(\"../data/train_feature_all.hdf\")\n",
    "    data_train_feature_all_no_neg = data_train_feature_all.loc[data_train_label_no_neg.index]\n",
    "    del data_train_feature_all\n",
    "\n",
    "    data_train_feature_all_no_neg[\"label\"] = data_train_label_no_neg\n",
    "    del data_train_label_no_neg\n",
    "\n",
    "    return data_train_feature_all_no_neg\n",
    "\n",
    "def sample_data(data, ratio=6):\n",
    "    logging.info(\"sample_data\")\n",
    "    train_1 = pd.concat([data[data[\"label\"] == 1]] * 5)\n",
    "    train_0 = data[data[\"label\"] == 0].sample(train_1.shape[0] * ratio)\n",
    "    del data\n",
    "    gc.collect()\n",
    "    train_all = pd.concat([train_1, train_0])\n",
    "    del train_1, train_0\n",
    "\n",
    "    return train_all\n",
    "\n",
    "\n",
    "def save_data(train_all):\n",
    "    logging.info(\"save_data\")\n",
    "    X = train_all.values[:, :-1].astype(np.float16)\n",
    "    y = train_all[\"label\"].values.astype(np.uint8)\n",
    "    del train_all\n",
    "    gc.collect()\n",
    "\n",
    "    np.save(\"../data/X_sample.npy\", X)\n",
    "    del X\n",
    "    gc.collect()\n",
    "    \n",
    "    np.save(\"../data/y_sample.npy\", y)\n",
    "    del y\n",
    "    gc.collect()\n",
    "\n",
    "# # float, int转换数据格式，降低内存\n",
    "# def dtype_descend(data):\n",
    "#     data_float = data.select_dtypes(include=['float'])\n",
    "#     data_converted_float = data_float.apply(pd.to_numeric, downcast='float')\n",
    "#     for column in data_converted_float.columns:\n",
    "#         print \"float \", column\n",
    "#         data[column] = data_converted_float[column]\n",
    "    \n",
    "#     data_int = data.select_dtypes(include=['int'])\n",
    "#     data_converted_int = data_int.apply(pd.to_numeric, downcast='unsigned')\n",
    "#     for column in data_converted_int.columns:\n",
    "#         print \"int \", column\n",
    "#         data[column] = data_converted_int[column]\n",
    "    \n",
    "#     return data\n",
    "    \n",
    "# 加载数据，转换数据\n",
    "def load_transfer_data():\n",
    "    logging.info(\"load_transfer_data\")\n",
    "    X = np.load(\"../data/X_sample.npy\")\n",
    "    y = np.load(\"../data/y_sample.npy\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)\n",
    "    del X\n",
    "    del y\n",
    "    np.save(\"../data/y_train.npy\", y_train)\n",
    "    np.save(\"../data/y_val.npy\", y_val)\n",
    "    del y_val, y_train\n",
    "    np.save(\"../data/X_train.npy\", X_train)\n",
    "    del X_train\n",
    "    np.save(\"../data/X_val.npy\", X_val)\n",
    "    del X_val\n",
    "    gc.collect()\n",
    "    \n",
    "    logging.info(\"dtrain\")\n",
    "    X_train = np.load(\"../data/X_train.npy\")\n",
    "    y_train = np.load(\"../data/y_train.npy\")\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, nthread=-1)\n",
    "    dtrain.save_binary(\"../data/dtrain.buffer\")\n",
    "    del dtrain, y_train\n",
    "    gc.collect()\n",
    "\n",
    "    logging.info(\"dtrain_X\")\n",
    "    dtrain_X = xgb.DMatrix(X_train, nthread=-1)\n",
    "    dtrain_X.save_binary(\"../data/dtrain_X.buffer\")\n",
    "    del X_train, dtrain_X\n",
    "    gc.collect()\n",
    "\n",
    "    logging.info(\"dval\")\n",
    "    X_val = np.load(\"../data/X_val.npy\")\n",
    "    dval = xgb.DMatrix(X_val, nthread=-1)\n",
    "    dval.save_binary(\"../data/dval.buffer\")\n",
    "    del X_val, dval\n",
    "    gc.collect()\n",
    "\n",
    "data = add_neg_data(30)\n",
    "train_all = sample_data(data)\n",
    "del data\n",
    "\n",
    "save_data(train_all)\n",
    "del train_all\n",
    "\n",
    "load_transfer_data()\n",
    "logging.info(\"end!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbf_sim.sort_values(by=\"rbf_sim\", inplace=True)\n",
    "\n",
    "# rbf_sim.to_pickle(\"../data/rbf_sim\")\n",
    "\n",
    "# 寻找十分位点\n",
    "# pd.qcut(rbf_sim[\"rbf_sim\"], 100).cat.categories.left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# print(sorted(Counter(train[\"label\"].values).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 这里随机采样的index就变了！十分注意！\n",
    "# from imblearn.combine import SMOTEENN\n",
    "# smote_enn = SMOTEENN(ratio={1:160000}, random_state=0)\n",
    "# X_resampled, y_resampled = smote_enn.fit_sample(train.values[:, :-1], train[\"label\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "# 整体统一训练\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy.interpolate import interp1d\n",
    "import logging\n",
    "import xgboost as xgb\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', filename='train.log',\n",
    "\t\t\t\t\tlevel=logging.INFO)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "def train(dtrain, max_depth=11, min_child_weight=1, gamma=0, eta=0.005, subsample=1, colsample_bytree=1,\n",
    "\t\t  num_round=1000, early_stopping_rounds=100,\n",
    "\t\t  scale_pos_weight=6, n_estimators=1000):\n",
    "\tlogging.info(\"train\")\n",
    "\n",
    "\tparam_prob = {'max_depth': max_depth, 'eta': eta, 'silent': False, 'tree_method': 'gpu_hist', 'eval_metric': 'auc',\n",
    "\t\t\t\t  'objective': 'binary:logistic', 'subsample': subsample, 'gamma': gamma,\n",
    "\t\t\t\t  'min_child_weight': min_child_weight,\n",
    "\t\t\t\t  'colsample_bytree': colsample_bytree, 'scale_pos_weight': scale_pos_weight,\n",
    "\t\t\t\t  'n_estimators': n_estimators}\n",
    "\tbst = xgb.train(params=param_prob, dtrain=dtrain, num_boost_round=num_round, evals=[(dtrain, \"train\")],\n",
    "\t\t\t\t\tearly_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "\tdel param_prob, dtrain\n",
    "\n",
    "\treturn bst\n",
    "\n",
    "\n",
    "def get_score(y_true, y_score):\n",
    "\tlogging.info(\"get_score\")\n",
    "\t#     print(\"auc:\", roc_auc_score(y_true, y_score))\n",
    "\tfpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "\tdel thresholds\n",
    "\n",
    "\tf = interp1d(fpr, tpr)\n",
    "\tscore = 0.4 * f(0.001) + 0.3 * f(0.005) + 0.3 * f(0.01)\n",
    "\n",
    "\tdel fpr, tpr\n",
    "\n",
    "\tlogging.info(\"score: %s\" % str(score))\n",
    "\n",
    "\tdel score\n",
    "\n",
    "\n",
    "# 单独一轮训练\n",
    "def train_single(max_depth=11, eta=0.005, subsample=1, num_round=600, scale_pos_weight=6, n_estimators=500):\n",
    "\tmodel_name = \"model_0611\"\n",
    "\tlogging.info(\"train_single\")\n",
    "\tdtrain = xgb.DMatrix('../data/dtrain.buffer')\n",
    "\tbst = train(dtrain=dtrain, max_depth=max_depth, eta=eta, subsample=subsample, num_round=num_round,\n",
    "\t\t\t\tscale_pos_weight=scale_pos_weight, n_estimators=n_estimators)\n",
    "\tdel dtrain\n",
    "\n",
    "\t# memory and gpu memory release\n",
    "\tbst.save_model(model_name)\n",
    "\tbst.__del__()\n",
    "\tdel bst\n",
    "\n",
    "\tbst = xgb.Booster({'nthread': 4})  # init model\n",
    "\tbst.load_model(model_name)  # load data\n",
    "\n",
    "\tdval = xgb.DMatrix('../data/dval.buffer')\n",
    "\ty_val_pred = bst.predict(dval)\n",
    "\tdel dval\n",
    "\n",
    "\tdtrain_X = xgb.DMatrix('../data/dtrain_X.buffer')\n",
    "\ty_train_pred = bst.predict(dtrain_X)\n",
    "\tdel dtrain_X, bst\n",
    "\n",
    "\ty_val = np.load(\"../data/y_val.npy\")\n",
    "\tget_score(y_val, y_val_pred)\n",
    "\tdel y_val\n",
    "\n",
    "\ty_train = np.load(\"../data/y_train.npy\")\n",
    "\tget_score(y_train, y_train_pred)\n",
    "\tdel y_train\n",
    "\n",
    "\n",
    "# # max_depth\n",
    "# for i in np.arange(6, 12, 1):\n",
    "# \tlogging.info(\"max_depth: %s\" % str(i))\n",
    "# \ttrain_single(max_depth=i)\n",
    "\n",
    "# # scale_pos_weight\n",
    "# for i in np.arange(1, 11, 1):\n",
    "# \tlogging.info(\"scale_pos_weight: %s\" % str(i))\n",
    "# \ttrain_single(scale_pos_weight=i)\n",
    "\n",
    "# # num_round\n",
    "# for i in np.arange(500, 1600, 100):\n",
    "# \tlogging.info(\"num_round: %s\" % str(i))\n",
    "# \ttrain_single(num_round=i)\n",
    "\n",
    "# # n_estimators\n",
    "# for i in np.arange(100, 1100, 100):\n",
    "# \tlogging.info(\"n_estimators: %s\" % str(i))\n",
    "# \ttrain_single(n_estimators=i)\n",
    "\n",
    "# # eta\n",
    "# for i in [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3]:\n",
    "# \tlogging.info(\"eta: %s\" % str(i))\n",
    "# \ttrain_single(eta=i)\n",
    "\n",
    "# # subsample\n",
    "# for i in np.arange(0.4, 1.1, 0.1):\n",
    "# \tlogging.info(\"subsample: %s\" % str(i))\n",
    "# \ttrain_single(subsample=i)\n",
    "\n",
    "train_single()\n",
    "\n",
    "logging.info(\"end!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有数据，进行最后一轮训练加预测的时候用\n",
    "\n",
    "X = np.load(\"../data/X_sample.npy\").astype(np.float16)\n",
    "y = np.load(\"../data/y_sample.npy\").astype(np.float16)\n",
    "\n",
    "dtrain_all = xgb.DMatrix(X, label=y)\n",
    "del y\n",
    "dtrain_all.save_binary(\"dtrain_all.buffer\")\n",
    "del dtrain_all\n",
    "# dtrain_all_X = xgb.DMatrix(X)\n",
    "# del X\n",
    "# dtrain_all_X.save_binary(\"dtrain_all_X.buffer\")\n",
    "# del dtrain_all_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有数据训练，最终用来预测\n",
    "\n",
    "model_name = \"model_0612\"\n",
    "\n",
    "dtrain = xgb.DMatrix('dtrain_all.buffer')\n",
    "bst = train()\n",
    "del dtrain\n",
    "\n",
    "# memory and gpu memory release\n",
    "bst.save_model(model_name)\n",
    "bst.__del__()\n",
    "del bst\n",
    "\n",
    "bst = xgb.Booster({'nthread': 4})  # init model\n",
    "bst.load_model(model_name)  # load data\n",
    "\n",
    "dtrain_all_X = xgb.DMatrix('dtrain_all_X.buffer')\n",
    "y_train_pred = bst.predict(dtrain_all_X)\n",
    "del dtrain_all_X, bst\n",
    "\n",
    "y_train_all = np.load(\"../data/y_sample.npy\")\n",
    "get_score(y_train_all, y_train_pred)\n",
    "del y_train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_number\n",
    "for i in np.arange(60, 420, 20):\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "    gc.collect()\n",
    "    print \"drop_number: \", i\n",
    "    data = add_neg_data(i)\n",
    "    train_all = sample_data(data)\n",
    "    del data\n",
    "\n",
    "    save_data(train_all)\n",
    "    del train_all\n",
    "\n",
    "    load_transfer_data()\n",
    "    train_single()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_pos_weight\n",
    "for i in np.arange(100, 500, 50):\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "    \n",
    "    scale_pos_weight = i\n",
    "    print \"scale_pos_weight\", scale_pos_weight\n",
    "\n",
    "    dtrain = xgb.DMatrix('dtrain.buffer')\n",
    "    bst = train(scale_pos_weight=scale_pos_weight)\n",
    "    del dtrain\n",
    "    \n",
    "    # memory and gpu memory release\n",
    "    bst.save_model('model_0602')\n",
    "    bst.__del__()\n",
    "    del bst\n",
    "    \n",
    "    bst = xgb.Booster({'nthread': 4})  # init model\n",
    "    bst.load_model('model_0602')  # load data\n",
    "\n",
    "    dval = xgb.DMatrix('dval.buffer')\n",
    "    y_val_pred = bst.predict(dval)\n",
    "    del dval\n",
    "\n",
    "    dtrain_X = xgb.DMatrix('dtrain_X.buffer')\n",
    "    y_train_pred = bst.predict(dtrain_X)\n",
    "    del dtrain_X, bst\n",
    "    \n",
    "    y_val = np.load(\"y_val.npy\")\n",
    "    get_score(y_val, y_val_pred)\n",
    "    del y_val\n",
    "    \n",
    "    y_train = np.load(\"y_train.npy\")\n",
    "    get_score(y_train, y_train_pred)\n",
    "    del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators\n",
    "for i in np.arange(100, 600, 100):\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "    \n",
    "    n_estimators = i\n",
    "    print \"n_estimators\", n_estimators\n",
    "\n",
    "    dtrain = xgb.DMatrix('dtrain.buffer')\n",
    "    bst = train(n_estimators=n_estimators)\n",
    "    del dtrain\n",
    "    \n",
    "    # memory and gpu memory release\n",
    "    bst.save_model('model_0602')\n",
    "    bst.__del__()\n",
    "    del bst\n",
    "    \n",
    "    bst = xgb.Booster({'nthread': 4})  # init model\n",
    "    bst.load_model('model_0602')  # load data\n",
    "\n",
    "    dval = xgb.DMatrix('dval.buffer')\n",
    "    y_val_pred = bst.predict(dval)\n",
    "    del dval\n",
    "\n",
    "    dtrain_X = xgb.DMatrix('dtrain_X.buffer')\n",
    "    y_train_pred = bst.predict(dtrain_X)\n",
    "    del dtrain_X, bst\n",
    "    \n",
    "    y_val = np.load(\"y_val.npy\")\n",
    "    get_score(y_val, y_val_pred)\n",
    "    del y_val\n",
    "    \n",
    "    y_train = np.load(\"y_train.npy\")\n",
    "    get_score(y_train, y_train_pred)\n",
    "    del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth and num_round 联合调参\n",
    "for max_depth, num_round in itertools.product(np.arange(8, 11, 1), np.arange(700, 1700, 200)):\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "    \n",
    "    max_depth = max_depth\n",
    "    num_round = num_round\n",
    "    print \"max_depth\", max_depth\n",
    "    print \"num_round\", num_round\n",
    "\n",
    "    dtrain = xgb.DMatrix('dtrain.buffer')\n",
    "    bst = train(max_depth=max_depth, num_round=num_round)\n",
    "    del dtrain\n",
    "    \n",
    "    # memory and gpu memory release\n",
    "    bst.save_model('model_0604')\n",
    "    bst.__del__()\n",
    "    del bst\n",
    "    \n",
    "    bst = xgb.Booster({'nthread': 4})  # init model\n",
    "    bst.load_model('model_0604')  # load data\n",
    "\n",
    "    dval = xgb.DMatrix('dval.buffer')\n",
    "    y_val_pred = bst.predict(dval)\n",
    "    del dval\n",
    "\n",
    "    dtrain_X = xgb.DMatrix('dtrain_X.buffer')\n",
    "    y_train_pred = bst.predict(dtrain_X)\n",
    "    del dtrain_X, bst\n",
    "\n",
    "    y_val = np.load(\"y_val.npy\")\n",
    "    get_score(y_val, y_val_pred)\n",
    "    del y_val\n",
    "    \n",
    "    y_train = np.load(\"y_train.npy\")\n",
    "    get_score(y_train, y_train_pred)\n",
    "    del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample\n",
    "for i in np.arange(0.4, 1.1, 0.1):\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "    \n",
    "    subsample = i\n",
    "    print(\"subsample\", subsample)\n",
    "\n",
    "    dtrain = xgb.DMatrix('dtrain.buffer')\n",
    "    bst = train(subsample=subsample)\n",
    "    del dtrain\n",
    "    \n",
    "    # memory and gpu memory release\n",
    "    bst.save_model('model_0602')\n",
    "    bst.__del__()\n",
    "    del bst\n",
    "    \n",
    "    bst = xgb.Booster({'nthread': 4})  # init model\n",
    "    bst.load_model('model_0602')  # load data\n",
    "\n",
    "    dval = xgb.DMatrix('dval.buffer')\n",
    "    y_val_pred = bst.predict(dval)\n",
    "    del dval\n",
    "\n",
    "    dtrain_X = xgb.DMatrix('dtrain_X.buffer')\n",
    "    y_train_pred = bst.predict(dtrain_X)\n",
    "    del dtrain_X, bst\n",
    "    \n",
    "    y_val = np.load(\"y_val.npy\")\n",
    "    get_score(y_val, y_val_pred)\n",
    "    del y_val\n",
    "    \n",
    "    y_train = np.load(\"y_train.npy\")\n",
    "    get_score(y_train, y_train_pred)\n",
    "    del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample\n",
    "for i in np.arange(0.4, 1.1, 0.1):\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "    \n",
    "    subsample = i\n",
    "    print(\"subsample\", subsample)\n",
    "\n",
    "    dtrain = xgb.DMatrix('dtrain.buffer')\n",
    "    bst = train(subsample=subsample)\n",
    "    del dtrain\n",
    "    \n",
    "    # memory and gpu memory release\n",
    "    bst.save_model('model_0602')\n",
    "    bst.__del__()\n",
    "    del bst\n",
    "    \n",
    "    bst = xgb.Booster({'nthread': 4})  # init model\n",
    "    bst.load_model('model_0602')  # load data\n",
    "\n",
    "    dval = xgb.DMatrix('dval.buffer')\n",
    "    y_val_pred = bst.predict(dval)\n",
    "    del dval\n",
    "\n",
    "    dtrain_X = xgb.DMatrix('dtrain_X.buffer')\n",
    "    y_train_pred = bst.predict(dtrain_X)\n",
    "    del dtrain_X, bst\n",
    "    \n",
    "    y_val = np.load(\"y_val.npy\")\n",
    "    get_score(y_val, y_val_pred)\n",
    "    del y_val\n",
    "    \n",
    "    y_train = np.load(\"y_train.npy\")\n",
    "    get_score(y_train, y_train_pred)\n",
    "    del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth\n",
    "for i in np.arange(6, 13, 1):\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "    \n",
    "    max_depth = i\n",
    "    print(\"max_depth\", max_depth)\n",
    "\n",
    "    dtrain = xgb.DMatrix('dtrain.buffer')\n",
    "    bst = train(max_depth=max_depth)\n",
    "    del dtrain\n",
    "    \n",
    "    # memory and gpu memory release\n",
    "    bst.save_model('model_0602')\n",
    "    bst.__del__()\n",
    "    del bst\n",
    "    \n",
    "    bst = xgb.Booster({'nthread': 4})  # init model\n",
    "    bst.load_model('model_0602')  # load data\n",
    "\n",
    "    dval = xgb.DMatrix('dval.buffer')\n",
    "    y_val_pred = bst.predict(dval)\n",
    "    del dval\n",
    "\n",
    "    dtrain_X = xgb.DMatrix('dtrain_X.buffer')\n",
    "    y_train_pred = bst.predict(dtrain_X)\n",
    "    del dtrain_X, bst\n",
    "\n",
    "    y_val = np.load(\"y_val.npy\")\n",
    "    get_score(y_val, y_val_pred)\n",
    "    del y_val\n",
    "    \n",
    "    y_train = np.load(\"y_train.npy\")\n",
    "    get_score(y_train, y_train_pred)\n",
    "    del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_round\n",
    "for i in np.arange(500, 1600, 100):\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "    \n",
    "    num_round = i\n",
    "    print(\"num_round\", num_round)\n",
    "\n",
    "    dtrain = xgb.DMatrix('dtrain.buffer')\n",
    "    bst = train(num_round=num_round)\n",
    "    del dtrain\n",
    "    \n",
    "    # memory and gpu memory release\n",
    "    bst.save_model('model_0602')\n",
    "    bst.__del__()\n",
    "    del bst\n",
    "    \n",
    "    bst = xgb.Booster({'nthread': 4})  # init model\n",
    "    bst.load_model('model_0602')  # load data\n",
    "\n",
    "    dval = xgb.DMatrix('dval.buffer')\n",
    "    y_val_pred = bst.predict(dval)\n",
    "    del dval\n",
    "\n",
    "    dtrain_X = xgb.DMatrix('dtrain_X.buffer')\n",
    "    y_train_pred = bst.predict(dtrain_X)\n",
    "    del dtrain_X, bst\n",
    "\n",
    "    y_val = np.load(\"y_val.npy\")\n",
    "    get_score(y_val, y_val_pred)\n",
    "    del y_val\n",
    "    \n",
    "    y_train = np.load(\"y_train.npy\")\n",
    "    get_score(y_train, y_train_pred)\n",
    "    del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta\n",
    "for i in [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3]:\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "    \n",
    "    eta = i\n",
    "    print(\"eta\", eta)\n",
    "\n",
    "    dtrain = xgb.DMatrix('dtrain.buffer')\n",
    "    bst = train(eta=eta)\n",
    "    del dtrain\n",
    "    \n",
    "    # memory and gpu memory release\n",
    "    bst.save_model('model_0602')\n",
    "    bst.__del__()\n",
    "    del bst\n",
    "    \n",
    "    bst = xgb.Booster({'nthread': 4})  # init model\n",
    "    bst.load_model('model_0602')  # load data\n",
    "\n",
    "    dval = xgb.DMatrix('dval.buffer')\n",
    "    y_val_pred = bst.predict(dval)\n",
    "    del dval\n",
    "\n",
    "    dtrain_X = xgb.DMatrix('dtrain_X.buffer')\n",
    "    y_train_pred = bst.predict(dtrain_X)\n",
    "    del dtrain_X, bst\n",
    "\n",
    "    y_val = np.load(\"y_val.npy\")\n",
    "    get_score(y_val, y_val_pred)\n",
    "    del y_val\n",
    "    \n",
    "    y_train = np.load(\"y_train.npy\")\n",
    "    get_score(y_train, y_train_pred)\n",
    "    del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colsample_bytree\n",
    "for i in np.arange(0.4, 1.1, 0.1):\n",
    "    print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) \n",
    "    \n",
    "    colsample_bytree = i\n",
    "    print(\"colsample_bytree\", colsample_bytree)\n",
    "\n",
    "    dtrain = xgb.DMatrix('dtrain.buffer')\n",
    "    bst = train(colsample_bytree=colsample_bytree)\n",
    "    del dtrain\n",
    "    \n",
    "    # memory and gpu memory release\n",
    "    bst.save_model('model_0602')\n",
    "    bst.__del__()\n",
    "    del bst\n",
    "    \n",
    "    bst = xgb.Booster({'nthread': 4})  # init model\n",
    "    bst.load_model('model_0602')  # load data\n",
    "\n",
    "    dval = xgb.DMatrix('dval.buffer')\n",
    "    y_val_pred = bst.predict(dval)\n",
    "    del dval\n",
    "\n",
    "    dtrain_X = xgb.DMatrix('dtrain_X.buffer')\n",
    "    y_train_pred = bst.predict(dtrain_X)\n",
    "    del dtrain_X, bst\n",
    "\n",
    "    y_val = np.load(\"y_val.npy\")\n",
    "    get_score(y_val, y_val_pred)\n",
    "    del y_val\n",
    "    \n",
    "    y_train = np.load(\"y_train.npy\")\n",
    "    get_score(y_train, y_train_pred)\n",
    "    del y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参日志可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nohup.out\", 'r') as f:\n",
    "    t = []\n",
    "    keywords = [\"drop_number\", \"score\"]\n",
    "    for line in f.readlines():\n",
    "        for k in keywords:\n",
    "            if k in line and \"get_score\" not in line:\n",
    "                t.append(line.strip())\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"parameter_tuning_log.txt\", 'r') as f:\n",
    "    t = [_.strip() for _ in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = []\n",
    "for i in range(0, len(t), 3):\n",
    "    tt.append([t[i].split(\":\")[0].strip(), t[i].split(\":\")[1].strip(), t[i+1].split(\":\")[1].strip(), t[i+2].split(\":\")[1].strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tt:\n",
    "    print _ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB预测，拼接结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model_0611\"\n",
    "\n",
    "bst = xgb.Booster({'nthread': 4})  # init model\n",
    "bst.load_model(model_name)  # load data\n",
    "\n",
    "data_test_feature_all = pd.read_hdf(\"../data/test_feature_all.hdf\").values.astype(np.float16)\n",
    "gc.collect()\n",
    "\n",
    "logging.info(\"dtest\")\n",
    "dtest = xgb.DMatrix(data_test_feature_all)\n",
    "del data_test_feature_all\n",
    "\n",
    "logging.info(\"predict\")\n",
    "ypred = bst.predict(dtest)\n",
    "\n",
    "del bst\n",
    "\n",
    "np.save(\"../data/test_pred.npy\", ypred)\n",
    "\n",
    "ypred = np.load(\"../data/test_pred.npy\")\n",
    "\n",
    "data_test = pd.read_pickle(\"../data/raw_pickle/test_a\")\n",
    "\n",
    "res = pd.DataFrame()\n",
    "res[\"id\"] = data_test[\"id\"]\n",
    "res[\"score\"] = ypred\n",
    "\n",
    "res.to_csv(\"../data/res_0612.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "533px",
    "left": "110px",
    "top": "111px",
    "width": "298px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
